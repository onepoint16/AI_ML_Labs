{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Task 1 Lecture Examples\n",
    " The code in this notebook is closely aligned to the code shown in the lecture. \n",
    " Here, we use an artificial time series data set. This is based data is roughly based a sine wave. We run three Deep Learning based models on that using a sliding window approach:\n",
    " - Fully Connected\n",
    " - Convolutional (Operating in only 1 dimension, not like images where it is two dimensions) \n",
    " - Recurrent NN.\n",
    "\n",
    "**TASK**: Work through the code and try to understand it. After that, try the **Exercises** as written in the Lab Week 10 Instructions. \n",
    "\n",
    "**NOTE**: Some parts of the code are outlined with the keyword `ADVANCED CODE`. You do not need to try to understand what this part of the code does, simply read the comment next to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "# setting the random seeds\n",
    "tensorflow.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# turn scientific notation off\n",
    "#  this is useful for printing out arrays as otherwise they are printed similar to 1.23456789e+01 which is not very readable\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# setting the size of the plots\n",
    "plt.rcParams['figure.figsize'] = [16, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion and Prepraration\n",
    "Usually we would use the data from some source, but for this example we will generate it ourselves.\n",
    "We will generate a sine wave with some noise and an upward trend and use it as our univatirate time series data. If you run the code below, you will see how this data looks. The x axis is the time step and the y axis is the value of the sine wave at that time step. \n",
    "The goal is for the model to predict the next value in the sequence and learn to ignore the noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED CODE: create a time series with trend and seasonality (e.g. create artificial data)\n",
    "def create_time_series(length, trend, seasonality, noise):\n",
    "    time = np.arange(length)\n",
    "    season = seasonality * np.sin(time / seasonality)\n",
    "    trend = trend * time\n",
    "    noise = noise * np.random.randn(length)\n",
    "\n",
    "    # normalize\n",
    "    series = (season + trend + noise)\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    series = (series - mean) / std\n",
    "\n",
    "    return time, series\n",
    "\n",
    "# plot the time series data\n",
    "time, series = create_time_series(length=4000, trend=0.1, seasonality=50, noise=3)\n",
    "plt.plot(time, series)\n",
    "plt.legend(['Variable 1'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we covered varying ways to create predictions or to partition the data.\n",
    "For this example, we will use a train/val/test split. \n",
    "\n",
    "This works, because we a) have a lot of data and b) we are using a sliding window approach.\n",
    "\n",
    "If we had a much smaller dataset, it would be more beneficial to use a cross-validation approach, adapted for time series data.\n",
    "\n",
    "Recall, the sliding window approach is to use a set number of previous time steps to predict the next time step. The other approach we looked at in the lecture was to use **all** previous time steps to predict the next time step. The sliding window approach allows us to use the same number of time steps for all predictions, which is useful for the neural networks, including fully connected ones. The other approach would require us to use a different number of time steps for each prediction. This is not possible with fully connected networks. It also is more efficient and allows us also to make efficient use of the validation set. \n",
    "\n",
    "What we need to define is the *window size* that we want to look at in the past to predict the next time step.\n",
    "\n",
    "For example, if we have a sequence of 4 time steps such as [1, 2, 3, 4, 5], and we want to use the previous 2 time steps (e.g. window size = 2) to predict the next time step, we would have the following input/output pairs:\n",
    "- [1, 2] -> 3\n",
    "- [2, 3] -> 4\n",
    "- [3, 4] -> 5\n",
    "\n",
    "We can use the keras function timeseries_dataset_from_array to create the dataset for us, after we have extracted the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # simple train/val/test split. Note, you could use any arbitrary split you want which is suited for your problem.\n",
    " # we use all but the last 1000 time steps for training\n",
    "train_series = series[:-1000]\n",
    "# from the last 1000 steps we use the first 500 for validation\n",
    "val_series = series[-1000:-500]\n",
    "# and the last 500 for testing\n",
    "test_series = series[-500:]\n",
    "\n",
    "# Note: It is always useful to see with how many time steps you are working with in total and to print the shape of the data\n",
    "print(\"Series Length\")\n",
    "print(\"Train:\", train_series.shape)\n",
    "print(\"Val:\", val_series.shape)\n",
    "print(\"Test:\", test_series.shape)\n",
    "\n",
    "\n",
    "# the size of the window or length of the sequence we look at\n",
    "sequence_length = 45\n",
    "\n",
    "# labels : shifted by sequence length\n",
    "# what [sequence_length:] does is to start the labels at the index where the sequence ends\n",
    "# in the [1, 2, 3, 4, 5] example, the labels would be [3, 4, 5] as we use a window size of 2, therefore starting with the third element as the first label\n",
    "train_labels = train_series[sequence_length:]\n",
    "val_labels = val_series[sequence_length:]\n",
    "test_labels = test_series[sequence_length:]\n",
    "\n",
    "# limit the training series to all but the last value, as this is the label and would otherwise cause issues with the RNN. \n",
    "train_series = train_series[:-1]\n",
    "\n",
    "\n",
    "# Datasets for tensorflow: https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array \n",
    "# A batch size of 8 is used and the sampling rate is 1, meaning that we use every time step in the sequence\n",
    "train_dataset = timeseries_dataset_from_array(train_series, train_labels, \n",
    "        sequence_length=sequence_length, sampling_rate=1, \n",
    "        batch_size=8, shuffle=True)\n",
    "val_dataset = timeseries_dataset_from_array(val_series, val_labels,\n",
    "        sequence_length=sequence_length, sampling_rate=1,\n",
    "        batch_size=8, shuffle=True)\n",
    "test_dataset = timeseries_dataset_from_array(test_series, test_labels,\n",
    "        sequence_length=sequence_length, sampling_rate=1,\n",
    "         batch_size=8)\n",
    "\n",
    "# Let's look at the first sample of the training dataset\n",
    "for batch in train_dataset.take(1):\n",
    "    inputs, targets = batch\n",
    "    print(\"Input shape:\", inputs.numpy().shape)\n",
    "    print(\"Target shape:\", targets.numpy().shape)\n",
    "    print(\"Input Sample:\", inputs.numpy()[0])\n",
    "    print(\"Input Sample Label:\", targets.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED CODE: printing an example of the model's predictions based on a random sample\n",
    "def print_prediction(model, dataset):\n",
    "    \"\"\"prints the model's predictions for a random sample from the dataset\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Model that creates the prediction\n",
    "        dataset (tensorflow.datasets.dataset): Dataset to sample from\n",
    "    \"\"\"\n",
    "    for batch in dataset.take(1):\n",
    "        inputs, targets = batch\n",
    "        print(\"Input shape:\", inputs.numpy().shape)\n",
    "        print(\"Target shape:\", targets.numpy().shape)\n",
    "        print(\"Input Sample:\", inputs.numpy()[0])\n",
    "        print(\"Input Sample Label:\", targets.numpy()[0])\n",
    "        print(\"Predictions:\", model.predict(inputs).flatten()[0])\n",
    "\n",
    "# ADVANCED CODE: plotting the model's predictions for a certain number of steps\n",
    "def plot_results(model, test_series, sequence_length, n=1):\n",
    "    \"\"\"Plots the model's predictions for the next n steps\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Model that creates the prediction\n",
    "        test_series (numpy.array): Test data\n",
    "        sequence_length (int): Length of the sequence that the model uses to predict the next step\n",
    "        n (int, optional): Number of steps to predict. Defaults to 1.\n",
    "    \"\"\"\n",
    "    if n > len(test_series) - sequence_length:\n",
    "        n = len(test_series) - sequence_length\n",
    "\n",
    "    time = np.arange(len(test_series[:sequence_length+n]))\n",
    "    forecast = []\n",
    "    for step in range(n):       \n",
    "        # create the sequence from the test series of length sequence_length\n",
    "        test_d = np.expand_dims(test_series[step:step + sequence_length], axis=[0, -1])\n",
    "        # forecast the next step\n",
    "        prediction = model.predict(test_d, verbose=0)\n",
    "\n",
    "        forecast.append(prediction)\n",
    "        model.reset_states()\n",
    "    \n",
    "    # plot test series\n",
    "    plt.plot(time, test_series[:sequence_length+n], label='Test Series')\n",
    "    plt.plot(time[sequence_length:], np.squeeze(forecast), label=\"Forecast\")\n",
    "    \n",
    "    return (time, forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network\n",
    "\n",
    "Fully connected neural networks can be used for time series forecasting, however they are not the best choice. This is because they do not take into account the sequentail nature of the data, meaning they weigh all time steps equally.\n",
    "In this example, we use a simple network that takes the sequence as an input, then has 128 fully connected units before passing it to the single output unit. Note that we do not use an output activation. This is because in our example here, we have a regresssion problem: We want to predict a continous valued number. \n",
    "\n",
    "We use the huber loss, as it is a very robust regression loss. You can read more about it [here](https://en.wikipedia.org/wiki/Huber_loss). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape is set to be the length of the sequence\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\", input_shape=[sequence_length]),\n",
    "    layers.Dense(1)])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss=\"huber\", optimizer=\"adam\")\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset)    \n",
    "\n",
    "# refit the model on the entire training dataset\n",
    "history = model.fit(train_dataset.concatenate(val_dataset), epochs=10)\n",
    "\n",
    "\n",
    "print_prediction(model, test_dataset)\n",
    "time, fc_predictions = plot_results(model, test_series, sequence_length, n=500) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "This model example uses a convolutional layer. Remember, convolutional layers slide a window over the input data and apply a function to the window. This is very helpful with sequential data as different time steps within the sequence are taken into account. It can therefore model dependencies between time steps. This example simply uses a single convolutional layer with a window size of 7 and a stride of 1. It applies 64 filters to the input data before we pass its output to a dense layer.\n",
    "\n",
    "We need to reshape the input data to be 3-dimensional as the convolutional layer expects a 3-dimensional input, e.g. (batch_size, sequence_length, number of features).  In our case, we only have one feature, so the input shape is (batch_size, sequence_length, 1), with the batch size being ignored in the input shape parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Input(shape=[sequence_length, 1]),\n",
    "    layers.Conv1D(filters=64, kernel_size=7, strides=1, activation=\"relu\"),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1)])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss=\"huber\", optimizer=\"adam\")\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, verbose=1)\n",
    "\n",
    "# refit the model on the entire training dataset\n",
    "history = model.fit(train_dataset.concatenate(val_dataset), epochs=10)\n",
    "\n",
    "\n",
    "print_prediction(model, test_dataset)\n",
    "_ , cnn_predictions = plot_results(model, test_series, sequence_length, n=500) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "The last model is a recurrent neural network with a single LSTM layer. The LSTM layer takes the whole sequence as input. It is much more powerful than the previous models, but it is also much slower to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.LSTM(32, input_shape=[sequence_length, 1]),\n",
    "    layers.Dense(1)])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=\"huber\", optimizer=\"adam\")\n",
    "\n",
    "# refit the model on the entire training dataset\n",
    "history = model.fit(train_dataset.concatenate(val_dataset), epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, verbose=1)\n",
    "_ , rnn_predictions = plot_results(model, test_series, sequence_length, n=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing The Results\n",
    "We can then compare the results of each model to visualize the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean average percentage error\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "print(test_series.shape)\n",
    "print(np.array(fc_predictions).shape)\n",
    "\n",
    "fc_mape = mape(test_series[sequence_length:], np.squeeze(fc_predictions))\n",
    "cnn_mape = mape(test_series[sequence_length:], np.squeeze(cnn_predictions))\n",
    "rnn_mape = mape(test_series[sequence_length:], np.squeeze(rnn_predictions))\n",
    "\n",
    "print(\"Fully Connected mape:\", fc_mape)\n",
    "print(\"CNN mape:\", cnn_mape)\n",
    "print(\"RNN mape:\", rnn_mape)\n",
    "\n",
    "plt.plot((time), test_series, label='Test Series')\n",
    "plt.plot(time[sequence_length:], np.squeeze(fc_predictions), label=\"FC Forecast\")\n",
    "plt.plot(time[sequence_length:], np.squeeze(cnn_predictions), label=\"CNN Forecast\")\n",
    "plt.plot(time[sequence_length:], np.squeeze(rnn_predictions), label=\"RNN Forecast\")\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('AIML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977335c72b126e3991b9de8b6fc74c7a8bf9097191ab51ecd2769bb8eacdf950"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
