{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Week 8 Extra Task Code\n",
    "This extra code is used to show how semantic segmentation can be run on a small medical dataset of retina vessels (CHASE).\n",
    "Semantic segmentation is the task of assigning a class label to every pixel in an image. For this use case, we have images of a retina, which is the part of the eye that contains the light-sensitive cells. The goal is to find all the pixels, that belong to blood vessels. This can aid specialists in the diagnosis and monitoring of issues within the eye.\n",
    "\n",
    "<img src=https://blogs.kingston.ac.uk/retinal/files/2016/11/cropped-retina6.jpg width=\"600\">\n",
    "\n",
    "Image Source: https://blogs.kingston.ac.uk/retinal/files/2016/11/cropped-retina6.jpg  \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The dataset source is as follows:\n",
    "- Fraz, Muhammad Moazam [Creator], Remagnino, Paolo, Hoppe, Andreas, Uyyanonvara, Bunyarit, Rudnicka, Alicja R [Creator], Owen, Christopher G [Creator] and Barman, Sarah A [Creator] (2012) CHASE_DB1 retinal vessel reference dataset. [Data Collection] [Link](https://blogs.kingston.ac.uk/retinal/chasedb1/).\n",
    "\n",
    "\n",
    "The dataset has been modified for this exercise and is provided on GCU learn: **CHASEDB1.zip**. Download and extract this file into the same directory as this Jupyter Notebook.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**TASK**: You are required to look up any function calls that are unclear to you to understand them: https://www.tensorflow.org/api_docs/python/tf/keras\n",
    "\n",
    "**NOTE**: Some parts of the code are outlined with the keyword `ADVANCED CODE`. You do not need to try to understand what this part of the code does, simply read the comment next to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the dataset train and test paths\n",
    "train_img_path = './CHASEDB1/train_images/'\n",
    "train_label_path = './CHASEDB1/train_labels/'\n",
    "\n",
    "test_img_path = './CHASEDB1/test_images/'\n",
    "test_label_path = './CHASEDB1/test_labels/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED CODE: function to show the image, label and optional prediction\n",
    "def show_image(img, label, pred=None):\n",
    "\n",
    "    img = img * 255\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title('Image')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(label, cmap='gray')\n",
    "    plt.title('Label')\n",
    "    if pred is not None:\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred, cmap='gray')\n",
    "        plt.title('Prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using image data generator to load the images and labels from the directory\n",
    "# https://stackoverflow.com/questions/58050113/imagedatagenerator-for-semantic-segmentation \n",
    "\n",
    "# Using a small batch size to avoid memory issues\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "img_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.2, rotation_range=90, horizontal_flip=True, vertical_flip=True)\n",
    "label_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.2, rotation_range=90, horizontal_flip=True, vertical_flip=True)\n",
    "seed = 42\n",
    "\n",
    "train_img_gen = img_data_gen.flow_from_directory(train_img_path, class_mode=None, seed=seed, target_size=(256, 256), batch_size=BATCH_SIZE, subset='training')\n",
    "train_label_gen = label_data_gen.flow_from_directory(train_label_path, class_mode=None, seed=seed, target_size=(256, 256),  batch_size=BATCH_SIZE, subset='training')\n",
    "train_gen = zip(train_img_gen, train_label_gen)\n",
    "\n",
    "val_img_gen = img_data_gen.flow_from_directory(train_img_path, class_mode=None, seed=seed, target_size=(256, 256),  batch_size=BATCH_SIZE, subset='validation')\n",
    "val_label_gen = label_data_gen.flow_from_directory(train_label_path, class_mode=None, seed=seed, target_size=(256, 256),  batch_size=BATCH_SIZE, subset='validation')\n",
    "val_gen = zip(val_img_gen, val_label_gen)\n",
    "\n",
    "\n",
    "test_img_gen = img_data_gen.flow_from_directory(test_img_path, class_mode=None, seed=seed, target_size=(256, 256),  batch_size=BATCH_SIZE)\n",
    "test_label_gen = label_data_gen.flow_from_directory(test_label_path, class_mode=None, seed=seed, target_size=(256, 256),  batch_size=BATCH_SIZE)\n",
    "test_gen = zip(test_img_gen, test_label_gen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a sample from the training set\n",
    "img, label = next(train_gen)\n",
    "show_image(img[0]/255., label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net for Semantic Segmentation\n",
    "U-net is a fully convolutional network that can be used for image segmentation\n",
    "U-net is used in many applications for image segmentation such as medical imaging, satellite imagery, and more.\n",
    "It features an encoder and decoder, where the encoder is used to extract features from the image and the decoder is used to upsample the features to the original image size.\n",
    "Skip connections are used to preserve the spatial information from the encoder to the decoder.\n",
    "The output is an image with the same size as the input, where each pixel is a class label.\n",
    "\n",
    "- Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015. ([Link to the original paper](https://arxiv.org/abs/1505.04597))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build a u-net keras model and code to compile it\n",
    "# not the non-specified input size. This allows for any size input. \n",
    "# The model will be trained on 256x256 images, but it can be used on any size image.\n",
    "\n",
    "# NOTE: This function uses keras functional API, which differs from the sequential API that you have been using so far. (https://keras.io/guides/functional_api/)\n",
    "\n",
    "def build_model():\n",
    "    inputs = layers.Input(shape=(None, None, 3))\n",
    "\n",
    "    # downsample\n",
    "    x1 = layers.Conv2D(16, 3, padding='same', activation='relu')(inputs)\n",
    "    x2 = layers.MaxPool2D()(x1)\n",
    "    x2 = layers.Conv2D(32, 3, padding='same', activation='relu')(x2)\n",
    "    x3 = layers.MaxPool2D()(x2)\n",
    "    x3 = layers.Conv2D(64, 3, padding='same', activation='relu')(x3)\n",
    "    x4 = layers.MaxPool2D()(x3)\n",
    "    x4 = layers.Conv2D(128, 3, padding='same', activation='relu')(x4)\n",
    "\n",
    "    # upsample\n",
    "    x = layers.UpSampling2D()(x4)\n",
    "    x = layers.Concatenate()([x, x3])\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    x = layers.Concatenate()([x, x2])\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    x = layers.Concatenate()([x, x1])\n",
    "    x = layers.Conv2D(16, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "    # output\n",
    "    outputs = layers.Conv2D(1, 1, padding='same', activation='sigmoid')(x)\n",
    "\n",
    "    # build functional Model\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = build_model()\n",
    "\n",
    "# compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to train the model.\n",
    "Depending on your computer hardware, this can be faster or slower.\n",
    "**TASK**: Try starting with a single epoch to see how training could take. Then set the value to be appropriate for your system. For this task you do not want it to take longer than about 2-3 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=1, steps_per_epoch=len(train_img_gen), validation_steps=len(val_img_gen))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training History\n",
    "Often you want to monitor the progress of your model training. This can be done via various helpful tools such as [Tensorboard](https://www.tensorflow.org/tensorboard) or very simple with plotting libraries as below.\n",
    "\n",
    "Monitoring the training process allows us to see if the model is overfitting or underfitting or if the training is progressing as expected. We can usually tell whether a model converges or not by looking at the training and validation loss and accuracy. \n",
    "The following \"rule of thumbs\" are a good guidance:\n",
    "- If the training loss does not decrease and the validation loss does not increase, then the model is not learning anything and seems to have converged at a local optimum. \n",
    "- If the training loss decreases and the validation loss increases, then the model is overfitting.\n",
    "- If the training loss decreases and the validation loss decreases, then the model is converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the focus of this tutorial is on the training process, not on the model performance or metrics. \n",
    "# This step is just to show that the model is able to predict something.\n",
    "model.evaluate(test_gen, steps=len(test_img_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "Once the model has been trained, we can then observe the prediciton outputs.\n",
    "As this is just to show you how few lines of code you need to implement a semantic segmentaiton model, we will not focus on improving performance or validation metrics for segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show samples from the test set\n",
    "# This code loops through the test set and shows the image, label, and prediction for each image.\n",
    "for _ in range(len(test_img_gen)):\n",
    "    img, label = next(test_gen)\n",
    "    pred = model.predict(img, verbose=0)\n",
    "    show_image(img[0]/255., label[0], pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Computer Vision Examples\n",
    "A list of furhter Computer Vision examples using different datasets and models can be found here:\n",
    " - U-Net for Pet Segmentaiton https://keras.io/examples/vision/oxford_pets_image_segmentation/\n",
    " - U-Net for Brain Tumor Segmentation (3D Data) https://keras.io/examples/vision/3D_image_classification/\n",
    " - Classificaiton network from scratch https://keras.io/examples/vision/image_classification_from_scratch/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "981baf1948874b62570d8088db1242b0b85753f221a31816c8c587837181a860"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
