{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Week 8 Task 2 Code\n",
    "This task is concerned with creating a dog/horse classifier using the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset.\n",
    "\n",
    "This is a bit more advanced than task 1 as it will show an example of how custom training works in neural networks, rather than just calling the *model.fit()* method.\n",
    "\n",
    "\n",
    "\n",
    "**TASK**: You are required to look up any function calls that are unclear to you to understand them: https://www.tensorflow.org/api_docs/python/tf/keras\n",
    "\n",
    "\n",
    "**NOTE**: Some parts of the code are outlined with the keyword `ADVANCED CODE`. You do not need to try to understand what this part of the code does, simply read the comment next to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR 10 is a dataset of 60,000 32x32pixel color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images.\n",
    "\n",
    "Therefore, the dataset is evently distributed among the 10 classes.\n",
    "\n",
    "We are using a dataset with small images to enable training on your computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# depending on your internet connection, this may take a while\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the class names to label the images as the dataset does not contain class names.\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "\n",
    "# notice that we have 3 channels, one for each color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows samples of the images in the original dataset with their corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED CODE: Plot the 16 random images from the training set and display the class name below each image.\n",
    "\n",
    "def sample_images(images, labels, class_names):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "\n",
    "    # get 16 random numbers between 0 and the number of images\n",
    "    random_indices = np.random.randint(0, len(images), 16)\n",
    "    \n",
    "\n",
    "    for i in range(16):\n",
    "        image_ix = random_indices[i]\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.subplots_adjust(hspace=.3)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[image_ix], cmap=plt.cm.binary)\n",
    "        plt.title(class_names[labels[image_ix][0]])\n",
    "    plt.show()\n",
    "\n",
    "sample_images(train_images, train_labels, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horses vs Dogs Classifier\n",
    "\n",
    "For this use-case we simply want to build a Horse-Dog classifier. \n",
    "The following cell contains code that extracts all the horses and all the dogs, from both the training and testing dataset and creates the new datasets with only those two classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED CODE: Extract all images with horses and dogs from the training set and testing set and use them as our new training and testing set.\n",
    "\n",
    "# filter out the images and labels for just horses and dogs\n",
    "horse_images_train = train_images[train_labels[:, 0] == 7]\n",
    "horse_labels_train = train_labels[train_labels[:, 0] == 7]\n",
    "dog_images_train = train_images[train_labels[:, 0] == 5]\n",
    "dogs_labels_train = train_labels[train_labels[:, 0] == 5]\n",
    "\n",
    "horse_images_test = test_images[test_labels[:, 0] == 7]\n",
    "horse_labels_test = test_labels[test_labels[:, 0] == 7]\n",
    "dog_images_test = test_images[test_labels[:, 0] == 5]\n",
    "dogs_labels_test = test_labels[test_labels[:, 0] == 5]\n",
    "\n",
    "# combine the horse and dog images into a single array for train and test\n",
    "train_images = np.concatenate((horse_images_train, dog_images_train))\n",
    "test_images = np.concatenate((horse_images_test, dog_images_test))\n",
    "\n",
    "# combine the horse and dog labels into a single array for train and test\n",
    "train_labels = np.concatenate((horse_labels_train, dogs_labels_train))\n",
    "test_labels = np.concatenate((horse_labels_test, dogs_labels_test))\n",
    "\n",
    "# create a new array of labels that are either 0 or 1\n",
    "# 0 for horse and 1 for dog\n",
    "train_labels = np.where(train_labels == 7, 0, 1)\n",
    "test_labels = np.where(test_labels == 7, 0, 1)\n",
    "\n",
    "class_names = ['horse', 'dog']\n",
    "\n",
    "sample_images(train_images, train_labels, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images need to be rescaled to be between 0 and 1 before being fed into the neural network. This is necessary because the pixel values are currently between 0 and 255 and neural networks work better with smaller input values, therefore we divide the values (which were between 0-255) by 255. \n",
    "\n",
    "**NOTE**: Do not rescale the labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set a random seed so that the results are reproducible\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "**TASK**: Use Task 1 as inspiration or your knowledge from last week to build a small, convolutional neural network following this specification:\n",
    "\n",
    "\n",
    "- Convolutional Layer, 3x3 Kernel, 32 Filters, 32px * 32px * 3 Input Size\n",
    "- ReLu Activation\n",
    "- Max Pooling Operation\n",
    "- Convolutional Layer, 3x3 Kernel, 32 Filters\n",
    "- ReLu Activation\n",
    "- Max Pooling Operation\n",
    "- Convolutional Layer, 3x3 Kernel, 32 Filters\n",
    "- ReLu Activation\n",
    "- Global Average Pooling Operation [Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D). (This layer combines each feature channel into a single value by averaging it).\n",
    "- Fully Connected Layer, 64 Units\n",
    "- ReLu Activation\n",
    "- Fully Connected Layer, 2 Units\n",
    "- Softmax Activation\n",
    "\n",
    "Then print the summary of this model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "\n",
    "\n",
    "model = # TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training with the model, you need to compile it and assign it a loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss = tf.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code fits the model to the training images for 10 epochs using a batch size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, batch_size=32, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model.evaluate() method checks the models performance\n",
    "However, as we have learned in the lecture about metrics, accuracy is not always the best metric to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('Accuracy on test set:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally we would like to also see the confusion matrix and further metrics.\n",
    "Code for this is implemented in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED CODE: Code to plot the confusion matrix and print the f1, precision, and recall scores\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_print_metrics(model, test_images, test_labels, class_names): \n",
    "    # create predictions\n",
    "    predictions = model.predict(test_images)\n",
    "\n",
    "\n",
    "\n",
    "    # get the predicted class as the index of the highest probability\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # calculate the precision, recall, and f1 score\n",
    "    precision = precision_score(test_labels, predicted_classes)\n",
    "    recall = recall_score(test_labels, predicted_classes)\n",
    "    f1 = f1_score(test_labels, predicted_classes)\n",
    "\n",
    "    # print the precision, recall, and f1 score\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1 Score:', f1)\n",
    "\n",
    "    # create the confusion matrix\n",
    "    cm = confusion_matrix(test_labels, predicted_classes)\n",
    "\n",
    "    # plot the confusion matrix\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_print_metrics(model, test_images, test_labels, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Training Process and Comparison with Augmentations\n",
    "\n",
    "Augmentations are a very strong tool that can be used in deep neural networks, especially with image data.\n",
    "There are several different ways on how to perform image augmentation, one was shown in the Task 1. \n",
    "The availablility of different augmentation libraries allows a high flexibilyt and adaptability to your use case. \n",
    "Popular libraries for this are:\n",
    "- PIL (Python Image Library) [Link](https://pillow.readthedocs.io/en/stable/)\n",
    "- Albumentations [Link](https://albumentations.ai/docs/)\n",
    "- Keras Preprocessing [Link](https://keras.io/guides/preprocessing_layers/)\n",
    "\n",
    "\n",
    "**TASK**: Ensure that you have installed the Albumentations library in your anaconda environment using the command: `pip install albumentations` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the albumentations library to perform data augmentation\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "# create a transform object that will perform the data augmentation\n",
    "# a list of abailable transforms can be found here: https://albumentations.ai/docs/api_reference/augmentations/transforms/\n",
    "# this randomly flips the image horizontally and vertically with a probability of 0.5\n",
    "# it also randomly shifts, resizes and rotates the image randomly up to 10° with a probability of 0.5\n",
    "# and it randomly cuts out a 2 portions of the image with a probability of 0.5\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(p=0.5, rotate_limit=10),\n",
    "    A.CoarseDropout(p=0.5, max_holes=2),\n",
    "])\n",
    "\n",
    "\n",
    "# ADVANCED CODE: A function that applies the transform with our augmentations to the image or multiple images\n",
    "def apply_augmentation(images, transform):\n",
    "    # function to apply the augmentation in the transform to the images\n",
    "\n",
    "    # this library needs images to be in range 0-255\n",
    "    images = (images * 255).astype(np.uint8)\n",
    "\n",
    "    if len(images.shape) == 4:\n",
    "        # if there are multiple images, apply the augmentation to each image\n",
    "        augmented_images = []\n",
    "        for image in images:\n",
    "            augmented_images.append(transform(image=image)['image'])\n",
    "        augmented_images = np.array(augmented_images)\n",
    "    else:\n",
    "        # if there is only one image, apply the augmentation to the image\n",
    "        augmented_images = transform(image=images)['image']\n",
    "        augmented_images = np.array(augmented_images)\n",
    "\n",
    "\n",
    "    # convert the images back to a range of 0-1\n",
    "    augmented_images = augmented_images / 255.\n",
    "\n",
    "    return augmented_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows some example code that shows examples of our proposed augmentation strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED CODE:  Function to show a random image from the training set augmented\n",
    "def show_augmentation(augmentation_function, original_image):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.subplots_adjust(hspace=.3)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        if i == 0:\n",
    "            augmented_image = original_image\n",
    "            plt.title('Original')\n",
    "        else:\n",
    "            plt.title('Augmented')\n",
    "            augmented_image = augmentation_function(original_image, transform)\n",
    "\n",
    "        plt.imshow(augmented_image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_augmentation(apply_augmentation, train_images[np.random.randint(0, len(train_images))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to recreate the model without copying the trained weights. The same settings as before are utilized to show a fair comparison with the non-augmented model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloning the model replicates the model architecture without the weights that have been trained in the previous cells\n",
    "model_aug = tf.keras.models.clone_model(model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss = tf.losses.SparseCategoricalCrossentropy()\n",
    "model_aug.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loop\n",
    "The following code shows how tensorflow and keras can be used to create a custom training loop.\n",
    "Remember from the deep learning lecture, during training we have:\n",
    "- Iterations: A single forward and backward pass of one or more samples through a neural network where the weights are updated\n",
    "- Batch: A number of paralell data samples that are used in an iteration\n",
    "- Epoch: One pass through the entire dataset using the batches. The total number of epochs is set by *you* but the number of iterations in an epoch is the number of samples in the dataset (`steps_per_epoch`), divided by the number of samples in a single batch. \n",
    "\n",
    "There is no right or wrong, whether you use a custom training loop as below or not. The example below offers you better capabilities, such as choosing your own augmentaiton provider. In most use cases you can however stick to the ones provied by the framework you are using.\n",
    "\n",
    "The example below is just to visualize (in code) how batches/epochs/iterations work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom keras training loop\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "steps_per_epoch = len(train_images) // batch_size\n",
    "\n",
    "# run the training process for the specified number of epochs\n",
    "for e in range(epochs):\n",
    "\n",
    "    # get random indexes to select random samples at each epoch\n",
    "    # instead of picking the samples directly, we pick the indexes which allow us to pick the correct samples and their labels. \n",
    "    indexes = np.random.permutation(len(train_images))\n",
    "\n",
    "    # keeping track of the loss and metric for this epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_metric = 0\n",
    "\n",
    "    # iterate over the indexes in batches\n",
    "    for i in range(0, len(indexes), batch_size):\n",
    "\n",
    "        # get the next batch of indexes\n",
    "        batch_indexes = indexes[i:i+batch_size]\n",
    "\n",
    "        # get the images and labels for the current batch\n",
    "        # indexing allows us to get the correct samples and their labels\n",
    "        batch_images = train_images[batch_indexes]\n",
    "        batch_labels = train_labels[batch_indexes]\n",
    "\n",
    "        # augment the images using your data augmentation pipeline and preferred strategy \n",
    "        # we are using albumentations\n",
    "        batch_images = apply_augmentation(batch_images, transform)\n",
    "\n",
    "        # train the model on this current batch\n",
    "        loss, metric = model_aug.train_on_batch(batch_images, batch_labels)\n",
    "\n",
    "        # update the epoch loss and metric by adding the loss and metric of the current batch\n",
    "        epoch_loss += loss\n",
    "        epoch_metric += metric\n",
    "\n",
    "    # print the loss and metric for this epoch \n",
    "    print(f'Epoch {e+1} completed', 'Epoch Loss:', epoch_loss/steps_per_epoch, 'Epoch Metric:', epoch_metric/steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_aug.evaluate(test_images, test_labels, verbose=2)\n",
    "\n",
    "print('Accuracy on test set:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare our *baseline* model with the *augmented* model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline Model\")\n",
    "plot_print_metrics(model, test_images, test_labels, class_names)\n",
    "print(\"Augmented Model\")\n",
    "plot_print_metrics(model_aug, test_images, test_labels, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Exercises\n",
    "3. Read the guide on how to create an augmentation pipeline from Albumentations [here](https://albumentations.ai/docs/examples/example/). Then add another sample augmentation such as RandomBrighntessContrast. If you retrain the models, does this change the performance?\n",
    "4. Try to use a larger model for training. For example, instead of only having a single convolutional layer, try using two with ReLU activations inbetween. This offers better feature extraction capabilities. Check if this improves performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "981baf1948874b62570d8088db1242b0b85753f221a31816c8c587837181a860"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
