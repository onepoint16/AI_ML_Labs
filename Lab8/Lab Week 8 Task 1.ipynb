{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Week 8 Task 1 Code\n",
    "Code Accompanying the Chapter 12: Categorizing Images of Clothing with Convolutional Neural Networks of the Book: **Python Machine Learning By Example by Yuxi Liu (3rd Ecition, 2020)**\n",
    "\n",
    "**TASK**: You are required to look up any function calls that are unclear to you to understand them: https://www.tensorflow.org/api_docs/python/tf/keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download fashion mnist dataset from tensorflow, split into training and testing sets and print the example labels that are numeric\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "print(train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of labels for the numeric labels to aid in plotting\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# print the shape of the training and testing sets to see the number of images and the size of the images\n",
    "# you will note that the images are 28x28 pixels and do not have a colour channel, meaning they are greyscale\n",
    "\n",
    "print(train_images.shape)\n",
    "\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code plots a random image from the training set and its label\n",
    "plt.figure()\n",
    "plt.imshow(train_images[42])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.title(class_names[train_labels[42]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the image values are between 0 and 255, we need to scale them to be between 0 and 1\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then display the first 16 images from the training set and their labels\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.subplots_adjust(hspace=.3)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.title(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the images to be 28x28x1 to be compatible with the convolutional layers\n",
    "# data fed into neural networks always need to have a batch dimension an when using images, commonly the batch dimension is the first dimension and the colour channel is the last dimension 4\n",
    "# the training shape then reflects the followin (number of images, height, width, colour channel(s))\n",
    "X_train = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
    "X_test = test_images.reshape((test_images.shape[0], 28, 28, 1))\n",
    "\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed assists in reproducibility\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sequential model and add the layers\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=losses.sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for 10 epochs (it is called iterations in the book) and print the accuracy and loss for the training and testing sets\n",
    "# NOTE: this will take a while to run and usually you DO NOT use the test set for validation, but we are doing it here for simplicity\n",
    "# Whilst this part of the code runs you should read the next parts in the book\n",
    "model.fit(X_train, train_labels, validation_data=(X_test, test_labels), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, test_labels, verbose=2)\n",
    "\n",
    "print('Accuracy on test set:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain the actual predicitons, we need to use the predict method\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# the output shows us the probability of each class for each image in scientific notation\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print('Predicted label for the first test sample: ', np.argmax(predictions[0]))\n",
    "print('True label for the first test sample: ',test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the probability of each class for the first test sample\n",
    "def plot_image_prediction(i, images, predictions, labels, class_names):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "    prediction = np.argmax(predictions[i])\n",
    "    color = 'blue' if prediction == labels[i] else 'red'\n",
    "    plt.title(f\"{class_names[labels[i]]} (predicted {class_names[prediction]})\", color=color)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plot = plt.bar(range(10), predictions[i], color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    plot[prediction].set_color('red')\n",
    "    plot[labels[i]].set_color('blue')\n",
    "    plt.show()\n",
    "\n",
    "plot_image_prediction(0, test_images, predictions, test_labels, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also visualise the convolutional layers, showing what kind of filters the network has learned\n",
    "\n",
    "filters, _ = model.layers[2].get_weights()\n",
    "\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "n_filters = 16\n",
    "for i in range(n_filters):\n",
    "    filter = filters[:, :, :, i]\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(filter[:, :, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting the CNN classifier with data augmentation\n",
    "Data augmentaiton aids us in increasing the training dataset size, which is achieved in the following code cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "def generate_plot_pics(datagen, original_img, save_prefix):\n",
    "    folder = 'aug_images'\n",
    "\n",
    "    # custom addition to make sure that the folder is created\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    for batch in datagen.flow(original_img.reshape((-1, 28, 28, 1)),\n",
    "                              batch_size=1,\n",
    "                              save_to_dir=folder,\n",
    "                              save_prefix=save_prefix,\n",
    "                              save_format='jpeg'):\n",
    "        i += 1\n",
    "        if i > 2:\n",
    "            break\n",
    "    plt.subplot(2, 2, 1, xticks=[],yticks=[])\n",
    "    plt.imshow(original_img)\n",
    "    plt.title(\"Original\")\n",
    "    i = 1\n",
    "    for file in os.listdir(folder):\n",
    "        if file.startswith(save_prefix):\n",
    "            plt.subplot(2, 2, i + 1, xticks=[],yticks=[])\n",
    "            aug_img = load_img(folder + \"/\" + file)\n",
    "            plt.imshow(aug_img)\n",
    "            plt.title(f\"Augmented {i}\")\n",
    "            i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "generate_plot_pics(datagen, train_images[0], 'horizontal_flip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             vertical_flip=True)\n",
    "generate_plot_pics(datagen, train_images[0], 'hv_flip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=30)\n",
    "generate_plot_pics(datagen, train_images[0], 'rotation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=8)\n",
    "generate_plot_pics(datagen, train_images[0], 'width_shift')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=8,\n",
    "                             height_shift_range=8)\n",
    "generate_plot_pics(datagen, train_images[0], 'width_height_shift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us test the augmentation on a small subset of the training set\n",
    "# we compare the performance of a model on the original training set and the augmented training set\n",
    "\n",
    "n_small = 500\n",
    "X_train = X_train[:n_small]\n",
    "train_labels = train_labels[:n_small]\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=losses.sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, train_labels, validation_data=(X_test, test_labels), epochs=20, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, test_labels, verbose=2)\n",
    "print('Accuracy on test set:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aug = tf.keras.models.clone_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aug.compile(optimizer='adam',\n",
    "              loss=losses.sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = datagen.flow(X_train, train_labels, seed=42, batch_size=40)\n",
    "model_aug.fit(train_generator, epochs=50, validation_data=(X_test, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_aug.evaluate(X_test, test_labels, verbose=2)\n",
    "print('Accuracy on test set:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "981baf1948874b62570d8088db1242b0b85753f221a31816c8c587837181a860"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
