{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Week 4 Content\n",
    "## (Lab Week 5 Content at the very end of this notebook)\n",
    "\n",
    "In the lecture we will have covered the different types of machine learning and crucial steps in a ML pipeline including ingestion of the data, preparation of the data for ML models and segregating it.\n",
    "\n",
    "In this lab you will:\n",
    "- Follow the first four ML pipeline steps of: Problem Definition, Data Ingestion, Data Preparation, Data Segratation of a dataset\n",
    "- Explore various feature extraction and engineering techniques to ready data for a model\n",
    "- Segregate (split) the data \n",
    "\n",
    "\n",
    "\n",
    "After this lab you will:\n",
    " - Understand how to work with new data in Python and ready it to make it suitable for a ML model\n",
    "\n",
    "\n",
    "Throughout this lab you will see various `#TODO` statements in code cells or the word **TASK** in Markdown cells. You will need to write the correct code for those tasks, as it is not provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline\n",
    "As described in the lecture, and following the book: *Artificial Intelligence with Python, Artasanchez, A., Joshi, P. (2020)* we have 8 important steps in a cyclical ML Pipeline:\n",
    "1) Problem Definition\n",
    "2) Data Ingestion\n",
    "3) Data Preparation\n",
    "4) Data Segregation\n",
    "5) Model Training\n",
    "6) Model Evaluation\n",
    "7) Model Deployment\n",
    "8) Performance Monitoring\n",
    "\n",
    "This lab will focus on the **first four**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem Definition\n",
    "Defining the problem is the first step in an ML pipeline. Without an actual problem, you would have no reason to set your ML pipeline up. \n",
    "\n",
    "In this lab, we will use [Kaggle](https://www.kaggle.com/competitions/titanic/overview) as a source and work with the Titanic Disaster Dataset. For us, therefore the problem definition can be extracted from this source as:\n",
    "\n",
    ">The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    ">\n",
    ">On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    ">\n",
    ">While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    ">\n",
    ">In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc). \n",
    "\n",
    "To narrow it down further, as we will only work with the first four steps in our ML pipeline we could write the problem formulation as: **Obtaining and preparing the Titanic Disaster Dataset so that it can be used in ML model training to determine if passengers were likely to survive given their data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Imports\n",
    "Before working with the data one needs to import all relevant Python modules. It is good practice to do that in a single Jupyter Notebook cell at the top of your code, rather than spread throughout the whole file.\n",
    "\n",
    "**TASK**: Modify your conda environment so that the modules below all have been installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Ingestion\n",
    "Data can be loaded from different sources, or you will need to collect new data based on your problem statement. In this case it should be relatively simple as in competitions such as on Kaggle, you have the data mostly available,  ready to be downloaded. \n",
    "Other times you may need to build connections to databases or parse a lot of files such as images or logs from a filesystem. \n",
    "\n",
    "In our case we can download the *train* data from Kaggle via this [link](https://www.kaggle.com/competitions/titanic/data). If you do not have a kaggle account you can also download it from [GitHub](https://github.com/datasciencedojo/datasets/blob/master/titanic.csv). \n",
    "\n",
    "**TASK**: Download the .csv file and save it in the same directory as this notebook as *train.csv* Then use the code cell below and utilize the pandas library to load the csv file into a variable called *df*. Then print out the top 10 lines of the dataframe so that you can have a look at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750   NaN        S  \n",
       "8      2            347742  11.1333   NaN        S  \n",
       "9      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO \n",
    "df = pd.read_csv(\"./train.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a look at the data. What you should try to figure out now is what the data is about and what the type of data is. You could use the [data source](https://www.kaggle.com/competitions/titanic/data). to see if they have a data dictionary which could help you decipher the meaning of each cell, but in other applications, you may need to create your own. \n",
    "\n",
    "**TASK**: Use the following markdown cell to note down the columns that contain:\n",
    "\n",
    "- The target variable\n",
    "- Categorical data, split by whether they are *ordinal* or not\n",
    "- Numerical data, split by whether they are *discrete* or *continuous*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable\n",
    "- Survived\n",
    "\n",
    "### Categorical data\n",
    "- Survived\n",
    "- PClass (ordinal)\n",
    "- Sex\n",
    "- Embarked\n",
    "\n",
    "### Numerical data\n",
    "- PassengerId (discrete)\n",
    "- Age (continous)\n",
    "- Fare (continous)\n",
    "- SibSp (discrete)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Data Preparation\n",
    "\n",
    "Usually, this step goes hand in hand with the EDA. However, as EDA is not a focus of this module let us assume the following assumptions that have been determined by the EDA (some of them have been changed for this lab and may not directly correlate with the actual data though):\n",
    "- A majority of survivors were female\n",
    "- The *Name* column has further information including titles\n",
    "- The *PassengerID*, *Ticket* and *Cabin* have low correlations with other variables and can be removed\n",
    "- Missing values are in the *Age*, *Cabin* and *Embarked* column\n",
    "- The *Pclass* variable has a high correlation with the *Age* variable\n",
    "- The *Fare* is higher, the higher the *Pclass*'s\n",
    "- The *Fare* has some outliers with some extreme pricey tickets\n",
    "- *SibSp* and *Parch* do not have a high correlation with other variables but could be combined to obtain a family size\n",
    "- Passengers in cabins in the *pclass* had higher survival rates\n",
    "- There are only a few older passengers based on the *Age* variable\n",
    "\n",
    "\n",
    "In this case, based on the EDA, we want to perform the following:\n",
    "1. Remove unnecessary columns\n",
    "2. Handle missing values\n",
    "3. Engineer features that utilize title within *name*\n",
    "4. Handle the *Age* variable \n",
    "5. Handle the *Fare* variable\n",
    "6. Handle the *SibSp* and *Parch* values to create a family size. \n",
    "7. Encode the data\n",
    "\n",
    "This is not an exclusive list of steps but shows the most common steps in this stage of an ML pipeline. Often there are more variables and further insights with an EDA which require more feature extraction and engineering as well as data preparation steps. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Remove unnecessary columns\n",
    "Based on the information above, the columns *ticket* and *cabin* can be removed. We want to retain PassengerId for later use for now. \n",
    "\n",
    "**Task**: Drop those two columns from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass                     Name   Sex   Age  SibSp  \\\n",
       "0            1         0       3  Braund, Mr. Owen Harris  male  22.0      1   \n",
       "\n",
       "   Parch  Fare Embarked  \n",
       "0      0  7.25        S  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO drop the columns that are not needed: ticket and cabin\n",
    "df = df.drop(columns=['Ticket', 'Cabin'])\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handle missing values\n",
    "Based on the information above, missing values are in the *Age*, *Cabin* and *Embarked* columns. As we have dropped the *Cabin* column, only the *Age* and *Embarked* are relevant now. \n",
    "\n",
    "The majority of passengers embarked from the port in Cherbourg (\"C\"). \n",
    "\n",
    "**Task**: Replace the missing values for the *Embarked* Column with \"C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:Replace the missing values for the Embarked Column with \"C\"\n",
    "df['Embarked'] = df['Embarked'].fillna('C')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EDA has shown that the *Pclass* variabe has a high correlation with the *Age* variable. \n",
    "\n",
    "Therefore, simply replacing all empty values in this column with the median would not be sensible. First, because we have a few older people who could skew the mean and second, because we have the high correlation as described. It would be better to fill the age values based on the mean age in the *Pclass*. Additionally it would also be beneficial to group those by *Sex* as more females have survived than males. \n",
    "\n",
    "**TASK**: \n",
    "1. Find the median age for each *Sex* per *Pclass* \n",
    "2. Fill the missing values that correlate a specific *Sex* and *Pclass* with the values obtained in step 1.\n",
    "\n",
    "Tip: Use [df.groupby](https://www.w3schools.com/python/pandas/ref_df_groupby.asp), [df.fillna](https://www.w3schools.com/python/pandas/ref_df_fillna.asp) and [df.transform](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html) for this. Note: This is the most difficult task in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Williams, Mr. Charles Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Masselmani, Mrs. Fatima</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Emir, Mr. Farred Chehab</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>O'Dwyer, Miss. Ellen \"Nellie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Razi, Mr. Raihed</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>864</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Miss. Dorothy Edith \"Dolly\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>869</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>van Melkebeke, Mr. Philemon</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>879</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Laleff, Mr. Kristo</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                                      Name  \\\n",
       "5              6         0       3                          Moran, Mr. James   \n",
       "17            18         1       2              Williams, Mr. Charles Eugene   \n",
       "19            20         1       3                   Masselmani, Mrs. Fatima   \n",
       "26            27         0       3                   Emir, Mr. Farred Chehab   \n",
       "28            29         1       3             O'Dwyer, Miss. Ellen \"Nellie\"   \n",
       "..           ...       ...     ...                                       ...   \n",
       "859          860         0       3                          Razi, Mr. Raihed   \n",
       "863          864         0       3         Sage, Miss. Dorothy Edith \"Dolly\"   \n",
       "868          869         0       3               van Melkebeke, Mr. Philemon   \n",
       "878          879         0       3                        Laleff, Mr. Kristo   \n",
       "888          889         0       3  Johnston, Miss. Catherine Helen \"Carrie\"   \n",
       "\n",
       "        Sex  Age  SibSp  Parch     Fare Embarked  \n",
       "5      male  NaN      0      0   8.4583        Q  \n",
       "17     male  NaN      0      0  13.0000        S  \n",
       "19   female  NaN      0      0   7.2250        C  \n",
       "26     male  NaN      0      0   7.2250        C  \n",
       "28   female  NaN      0      0   7.8792        Q  \n",
       "..      ...  ...    ...    ...      ...      ...  \n",
       "859    male  NaN      0      0   7.2292        C  \n",
       "863  female  NaN      8      2  69.5500        S  \n",
       "868    male  NaN      0      0   9.5000        S  \n",
       "878    male  NaN      0      0   7.8958        S  \n",
       "888  female  NaN      1      2  23.4500        S  \n",
       "\n",
       "[177 rows x 10 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Find all values where the Age is NaN for checking whether your code is correct\n",
    "df[df['Age'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               PassengerId  Survived   Age  SibSp  Parch      Fare\n",
      "Sex    Pclass                                                     \n",
      "female 1             447.0       1.0  35.0    0.0    0.0  82.66455\n",
      "       2             439.5       1.0  28.0    0.0    0.0  22.00000\n",
      "       3             376.0       0.5  21.5    0.0    0.0  12.47500\n",
      "male   1             480.5       0.0  40.0    0.0    0.0  41.26250\n",
      "       2             416.5       0.0  30.0    0.0    0.0  13.00000\n",
      "       3             466.0       0.0  25.0    0.0    0.0   7.92500\n"
     ]
    }
   ],
   "source": [
    "#TODO: Find the median age, per *Pclass* and per *Sex* and fill the missing values that correlate a specific *Pclass* and *Sex* with the values obtained\n",
    "\n",
    "# df grouped by Sex and Pclass and median computed\n",
    "df_sex_pclass = df.groupby(by=['Sex', 'Pclass']).median(numeric_only=True)\n",
    "print(df_sex_pclass)\n",
    "\n",
    "# applied the median to the missing values\n",
    "df['Age'] = df['Age'].fillna(df.groupby(by=['Sex', 'Pclass'])['Age'].transform('median'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Engineer features that utilize a title within *name*\n",
    "The EDA has found that there are titles included in the names after the first comma, with examples being:\n",
    "- Braund, Mr. Owen Harris\n",
    "- Futrelle, Mrs. Jacques Heath (Lily May Peel)\n",
    "\n",
    "We want to extract this title and and out all the unique values of titles. It is also assumed that titles can be based on the Sex. \n",
    "\n",
    "**TASK**\n",
    "1. Run the code below and try to understand how the titles have been extracted. \n",
    "2. Create a new column *Title* using the query below\n",
    "3. Replace all occurences of the \"Ms\" and \"Mlle\" title with \"Miss\" and \"Mme\" with \"Mrs\".\n",
    "4. Replace all occurences of titles other than \"Mr\", \"Mrs\", \"Miss\" with \"Other\".\n",
    "5. Drop all the *Name* values as they are not necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr' 'Mrs' 'Miss' 'Master' 'Don' 'Rev' 'Dr' 'Mme' 'Ms' 'Major' 'Lady'\n",
      " 'Sir' 'Mlle' 'Col' 'Capt' 'th' 'Jonkheer']\n"
     ]
    }
   ],
   "source": [
    "print(df['Name'].str.split(', ', expand=True)[1].str.split('. ', expand=True)[0].unique())\n",
    "\n",
    "#TODO: create a new column called title in the df and use the query above\n",
    "df[\"Title\"] = df['Name'].str.split(', ', expand=True)[1].str.split('. ', expand=True)[0]\n",
    "\n",
    "#TODO: Replace all occurences of titles other than \"Mr\", \"Mrs\", \"Miss\" with \"Other\"\n",
    "df[\"Title\"] = df[\"Title\"].replace(['Don', 'Rev', 'Dr', 'Mme', 'Ms', 'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'the Countess', 'Jonkheer', 'Dona'], 'Other')\n",
    "\n",
    "#TODO: Drop all the *Name* values as they are not necessary\n",
    "df = df.drop(columns=['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Handle the *Age* variable \n",
    "The EDA has shown that there are only a few older passengers based on the *Age* variable.\n",
    "One of the techniques we could use to handle this continuous variable is to use scaling. Here we could simply scale the *Age* values to a range of between 0 and 1. \n",
    "\n",
    "However, children were likely given a higher priority on lifeboats, therefore creating age groups is more appropriate rather than scaling. Additionally, scaling may also produce incorrect results as there are only a few people of higher age. \n",
    "\n",
    "A technique that is used to group data is Binning. In Python Machine Learning by Example, Liu, Y., (2020), Chapter 1, it is described as:\n",
    "> Sometimes, it's useful to separate feature values into several bins. For example,\n",
    "we may only be interested in whether it rained on a particular day. Given the\n",
    "precipitation values, we can binarize the values, so that we get a true value if the\n",
    "precipitation value isn't zero, and a false value otherwise. We can also use statistics\n",
    "to divide values into high, low, and medium bins. In marketing, we often care more\n",
    "about the age group, such as 18 to 24, than a specific age, such as 23.\n",
    ">\n",
    "> The binning process inevitably leads to loss of information. However, depending on\n",
    "your goals, this may not be an issue, and actually reduces the chance of overfitting.\n",
    "Certainly, there will be improvements in speed and reduction of memory or storage\n",
    "requirements and redundancy.\n",
    "\n",
    "In our case we want to use the *Age* column and create an ordinal value out of this with three bins:\n",
    "- Low Age 0<=18Years\n",
    "- Medium Age 18<=65Years\n",
    "- High Age 65< Years\n",
    "\n",
    "**Task**: Replace the values in the *Age* column based on the abovenamed groups. Use the following ordinal values:\n",
    "    - Low Age : 1\n",
    "    - Medium Age : 2\n",
    "    - High Age: 3\n",
    "    \n",
    "Tip: Use [df.loc](https://www.w3resource.com/pandas/dataframe/dataframe-loc.php) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>Mrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>Mrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Q</td>\n",
       "      <td>Mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>S</td>\n",
       "      <td>Master</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>S</td>\n",
       "      <td>Mrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>C</td>\n",
       "      <td>Mrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass     Sex  Age  SibSp  Parch     Fare Embarked  \\\n",
       "0            1         0       3    male  2.0      1      0   7.2500        S   \n",
       "1            2         1       1  female  2.0      1      0  71.2833        C   \n",
       "2            3         1       3  female  2.0      0      0   7.9250        S   \n",
       "3            4         1       1  female  2.0      1      0  53.1000        S   \n",
       "4            5         0       3    male  2.0      0      0   8.0500        S   \n",
       "5            6         0       3    male  2.0      0      0   8.4583        Q   \n",
       "6            7         0       1    male  2.0      0      0  51.8625        S   \n",
       "7            8         0       3    male  1.0      3      1  21.0750        S   \n",
       "8            9         1       3  female  2.0      0      2  11.1333        S   \n",
       "9           10         1       2  female  1.0      1      0  30.0708        C   \n",
       "\n",
       "    Title  \n",
       "0      Mr  \n",
       "1     Mrs  \n",
       "2    Miss  \n",
       "3     Mrs  \n",
       "4      Mr  \n",
       "5      Mr  \n",
       "6      Mr  \n",
       "7  Master  \n",
       "8     Mrs  \n",
       "9     Mrs  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Replace the values in the *Age* column based on the abovenamed groups\n",
    "df.loc[df['Age'] <= 18, 'Age'] = 1\n",
    "df.loc[(df['Age'] > 18) & (df['Age'] <= 65), 'Age'] = 2\n",
    "df.loc[df['Age'] > 65, 'Age'] = 3\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Handle the *Fare* variable \n",
    "The *Fare* has some of the same properties as age with some pricy tickets and a lot of them being 'cheap'. \n",
    "\n",
    "**TASK**: Similar to the *Age* variable, create three bins for ticket prices. You can use the plot below as guidance or use other EDA techniques to examine the *fare* values and then decide on the ranges for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the distribution of data in the fare\n",
    "sns.displot(df['Fare'])\n",
    "\n",
    "#TODO: Replace the values in the *Fare* column based on three groups\n",
    "df.loc[df['Fare'] <= 10, 'Fare'] = 1\n",
    "df.loc[(df['Fare'] > 10) & (df['Fare'] <= 100), 'Fare'] = 2\n",
    "df.loc[df['Fare'] > 100, 'Fare'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Handle the *SibSp* and *Parch* values to create a family size\n",
    "\n",
    "The *SibSp* and *Parch* variables tell us something about the size of the family of a specific passenger aboard. However, as those values are recorded separately, we would need to combine them first. \n",
    "Use your skills obtained in the previous cells to do this.\n",
    "\n",
    "**TASK**:\n",
    "1) Create a *FamilySize* column in the dataframe by adding the *SibSp* and *Parch* variables together. (Do not forget to add +1 for the actual passenger)\n",
    "2) Drop the *SibSp* and *Parch* columns\n",
    "3) Create three groups via binning again for passengers travelling in a Family of Size: \n",
    "- 1 Person: Alone: Group 1\n",
    "- 2-4 Persons: Small Family: Group 2\n",
    "- 4< Persons, Lage Family: Group 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>FamilySize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>C</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>S</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass     Sex  Age  Fare Embarked Title  FamilySize\n",
       "0            1         0       3    male  2.0   1.0        S    Mr           2\n",
       "1            2         1       1  female  2.0   2.0        C   Mrs           2\n",
       "2            3         1       3  female  2.0   1.0        S  Miss           1\n",
       "3            4         1       1  female  2.0   2.0        S   Mrs           2\n",
       "4            5         0       3    male  2.0   1.0        S    Mr           1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Create a *FamilySize* column in the dataframe by adding the *SibSp* and *Parch* variables together. (Do not forget to add +1 for the actual passenger)\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "\n",
    "#TODO: Drop the *SibSp* and *Parch* columns\n",
    "df = df.drop(columns=['SibSp', 'Parch'])\n",
    "\n",
    "#TODO: Replace the values in the *FamilySize* column based on three groups\n",
    "df.loc[df['FamilySize'] <= 1, 'FamilySize'] = 1\n",
    "df.loc[(df['FamilySize'] > 1) & (df['FamilySize'] <= 4), 'FamilySize'] = 2\n",
    "df.loc[df['FamilySize'] > 4, 'FamilySize'] = 3\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Encode the data\n",
    "Encoding the data is the last step in the data preparation stage. \n",
    "Here, we are going to transform non-numerical values into numerical ones so that our models can, later on, learn from that data. \n",
    "\n",
    "The ordinal data that we have prepared does not need to be changed further. This includes the variables *Fare* and *Age* since they are naturally ordered and we have only grouped them. We have therefore indirectly performed label encoding (e.g. from categories to labels) already.\n",
    "\n",
    "**Label Encoding**: The remaining variable that needs to be label encoded is Sex. We can simply turn the various categorical values into numeric values by mapping female-->0 and male-->1.\n",
    "\n",
    "**One Hot Encoding**: The other remaining variables *Embarked*, *Pclass*, *Title* and *FamilySize* are not ordinal and have each different categories. Since the order of those categories does not matter and we do not have too many features, we can use One-Hot Encoding. For this, each unique value for each variable gets its own column. For example, we would turn *Embarked* and its three ports S, C and Q into *Embarked_C*, *Embarked_Q*, *Embarked_S*.\n",
    "\n",
    "**Task**:\n",
    "1) Use Label Encoding to encode the values in the *Sex* column in the dataframe. (The scikit-learn approach has been given, but also try the manual approach)\n",
    "2) Try to understand the code approach to One_Hot encoding below. Use the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) to aid you.\n",
    "\n",
    "Note: We do not encode the *survived* variable yet, as a) there are only two classes and b) it is the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Sex  Age  Fare  FamilySize  Embarked_C  Embarked_Q  \\\n",
      "0            1         0    1  2.0   1.0           2           0           0   \n",
      "1            2         1    0  2.0   2.0           2           1           0   \n",
      "2            3         1    0  2.0   1.0           1           0           0   \n",
      "3            4         1    0  2.0   2.0           2           0           0   \n",
      "4            5         0    1  2.0   1.0           1           0           0   \n",
      "5            6         0    1  2.0   1.0           1           0           1   \n",
      "6            7         0    1  2.0   2.0           1           0           0   \n",
      "7            8         0    1  1.0   2.0           3           0           0   \n",
      "8            9         1    0  2.0   2.0           2           0           0   \n",
      "9           10         1    0  1.0   2.0           2           1           0   \n",
      "\n",
      "   Embarked_S  Pclass_1  Pclass_2  Pclass_3  Title_Master  Title_Miss  \\\n",
      "0           1         0         0         1             0           0   \n",
      "1           0         1         0         0             0           0   \n",
      "2           1         0         0         1             0           1   \n",
      "3           1         1         0         0             0           0   \n",
      "4           1         0         0         1             0           0   \n",
      "5           0         0         0         1             0           0   \n",
      "6           1         1         0         0             0           0   \n",
      "7           1         0         0         1             1           0   \n",
      "8           1         0         0         1             0           0   \n",
      "9           0         0         1         0             0           0   \n",
      "\n",
      "   Title_Mr  Title_Mrs  Title_Other  Title_th  \n",
      "0         1          0            0         0  \n",
      "1         0          1            0         0  \n",
      "2         0          0            0         0  \n",
      "3         0          1            0         0  \n",
      "4         1          0            0         0  \n",
      "5         1          0            0         0  \n",
      "6         1          0            0         0  \n",
      "7         0          0            0         0  \n",
      "8         0          1            0         0  \n",
      "9         0          1            0         0  \n"
     ]
    }
   ],
   "source": [
    "# TODO: Manual approach to label encoding. \n",
    "# Create a new python dictionary called map_dct and use the keys female for 0 and male for 1\n",
    "# Then select the df column *sex* and apply this dictionary using the map function https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html\n",
    "map_dct = {\"female\":0, \"male\":1}\n",
    "df[\"Sex\"].map(map_dct)\n",
    "\n",
    "\n",
    "# Scikit-learn approach to label encoding. \n",
    "features_to_label_encode = [\"Sex\"]\n",
    "for f in features_to_label_encode:\n",
    "    df[f] = sklearn.preprocessing.LabelEncoder().fit_transform(df[f])\n",
    "\n",
    "\n",
    "# Pandas approach to one hot encoding\n",
    "features_to_onehot_encode = [\"Embarked\", \"Pclass\", \"Title\"]\n",
    "for f in features_to_onehot_encode:\n",
    "    df = pd.concat([df, pd.get_dummies(df[f], prefix=f)], axis=1)\n",
    "    df = df.drop(columns=[f])\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data isready to be used for the next step in the ML pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Data Segregation\n",
    "\n",
    "Once you have prepared the data, it should be segregated into a train/test or train/val/test split. \n",
    "The training set is used to develop and train your model, the test set is used to obtain the performance of the model. You do not want those sets to overlap, as a test set should be unbiased, e.g. it should be new data that your model has not seen before. \n",
    "\n",
    "If you want to quantify the performance of the model on unseen examples *without* needing to use the test set, you can also set aside a validation set. This gives you an option to determine the performance of a model and then change some model settings such as hyperparameters, retrain the train set and observe the performance again. Whilst the data from the validation set does not directly affect the training, this dataset becomes more biased the more you use it to tune the model which can still lead to some kind of overfitting. \n",
    "\n",
    "Sometimes, the test set is given (e.g. in Kaggle competitions or benchmark datasets), sometimes you get all the data from the start and need to create your own test set. \n",
    "\n",
    "The proportion of train/val/test data is not fixed and changes depending on various factors, such as total amount of data available and how much data is necessary for training a model. A good guideline is to look at what others have done with a specific dataset, like looking at scientific papers that also use this data. The common starting point is usually an 70/0/30 or 80/0/20 split for train/val/test if you do not want to use a validation set, or 70/10/20 if you use a validation set. \n",
    "\n",
    "Note: For sequential data (which will be introduced later) such as stock prices you would usually use different splits as you can't just leave out the validation set and directly run predictions on the test set. \n",
    "\n",
    "**TASK**: Split the dataframe into two separate parts using pandas. Select the first 80 percent and save them in a dataframe variable *train_df* and the latter 20% in a variable *test_df*. Then print out the first 10 lines of each subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Sex  Age  Fare  FamilySize  Embarked_C  Embarked_Q  \\\n",
      "0            1         0    1  2.0   1.0           2           0           0   \n",
      "1            2         1    0  2.0   2.0           2           1           0   \n",
      "2            3         1    0  2.0   1.0           1           0           0   \n",
      "3            4         1    0  2.0   2.0           2           0           0   \n",
      "4            5         0    1  2.0   1.0           1           0           0   \n",
      "5            6         0    1  2.0   1.0           1           0           1   \n",
      "6            7         0    1  2.0   2.0           1           0           0   \n",
      "7            8         0    1  1.0   2.0           3           0           0   \n",
      "8            9         1    0  2.0   2.0           2           0           0   \n",
      "9           10         1    0  1.0   2.0           2           1           0   \n",
      "\n",
      "   Embarked_S  Pclass_1  Pclass_2  Pclass_3  Title_Master  Title_Miss  \\\n",
      "0           1         0         0         1             0           0   \n",
      "1           0         1         0         0             0           0   \n",
      "2           1         0         0         1             0           1   \n",
      "3           1         1         0         0             0           0   \n",
      "4           1         0         0         1             0           0   \n",
      "5           0         0         0         1             0           0   \n",
      "6           1         1         0         0             0           0   \n",
      "7           1         0         0         1             1           0   \n",
      "8           1         0         0         1             0           0   \n",
      "9           0         0         1         0             0           0   \n",
      "\n",
      "   Title_Mr  Title_Mrs  Title_Other  Title_th  \n",
      "0         1          0            0         0  \n",
      "1         0          1            0         0  \n",
      "2         0          0            0         0  \n",
      "3         0          1            0         0  \n",
      "4         1          0            0         0  \n",
      "5         1          0            0         0  \n",
      "6         1          0            0         0  \n",
      "7         0          0            0         0  \n",
      "8         0          1            0         0  \n",
      "9         0          1            0         0  \n",
      "     PassengerId  Survived  Sex  Age  Fare  FamilySize  Embarked_C  \\\n",
      "712          713         1    1  2.0   2.0           2           0   \n",
      "713          714         0    1  2.0   1.0           1           0   \n",
      "714          715         0    1  2.0   2.0           1           0   \n",
      "715          716         0    1  2.0   1.0           1           0   \n",
      "716          717         1    0  2.0   3.0           1           1   \n",
      "717          718         1    0  2.0   2.0           1           0   \n",
      "718          719         0    1  2.0   2.0           1           0   \n",
      "719          720         0    1  2.0   1.0           1           0   \n",
      "720          721         1    0  1.0   2.0           2           0   \n",
      "721          722         0    1  1.0   1.0           2           0   \n",
      "\n",
      "     Embarked_Q  Embarked_S  Pclass_1  Pclass_2  Pclass_3  Title_Master  \\\n",
      "712           0           1         1         0         0             0   \n",
      "713           0           1         0         0         1             0   \n",
      "714           0           1         0         1         0             0   \n",
      "715           0           1         0         0         1             0   \n",
      "716           0           0         1         0         0             0   \n",
      "717           0           1         0         1         0             0   \n",
      "718           1           0         0         0         1             0   \n",
      "719           0           1         0         0         1             0   \n",
      "720           0           1         0         1         0             0   \n",
      "721           0           1         0         0         1             0   \n",
      "\n",
      "     Title_Miss  Title_Mr  Title_Mrs  Title_Other  Title_th  \n",
      "712           0         1          0            0         0  \n",
      "713           0         1          0            0         0  \n",
      "714           0         1          0            0         0  \n",
      "715           0         1          0            0         0  \n",
      "716           1         0          0            0         0  \n",
      "717           1         0          0            0         0  \n",
      "718           0         1          0            0         0  \n",
      "719           0         1          0            0         0  \n",
      "720           1         0          0            0         0  \n",
      "721           0         1          0            0         0  \n"
     ]
    }
   ],
   "source": [
    "# TODO split the dataset into 80% train and 20% test. Note: it is usually better to calculate the results automatically using percentages rather than manually selecting the top 80%.\n",
    "train_df = df[:int(0.8*df.shape[0])]\n",
    "test_df = df[int(0.8*df.shape[0]):]\n",
    "\n",
    "print(train_df.head(10))\n",
    "print(test_df.head(10))\n",
    "\n",
    "# Automatic Test to check whether your split was correct\n",
    "assert train_df.shape[0] >= 712 and train_df.shape[0] <= 713\n",
    "assert test_df.shape[0] >= 178 and test_df.shape[0] <= 179\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the PassengerId value, which we have not yet removed, comes into play. It nicely visualises that a basic split using standard indexing *may not be the best solution*. You can see that, even though the data was split the **first** 80% of passengers are in the train set. Optimally you would want to avoid any kind of ordering in the train/val/test data as there may be some bias attached to it, for example, that the first bit of tickets was only sold to people that reside in cabins that had a higher chance of survival (Note: Using EDA as taught in the SDfDS module could confirm that assumption, but we are focusing on data preparation in this module). \n",
    "\n",
    "Therefore, the best approach to alleviate this is to use random sampling. \n",
    "This is achieved by randomly shuffling the data and then extracting the train/val/test split proportions of data again. \n",
    "\n",
    "It is a good practice to use a random seed ( [Python](https://docs.python.org/3/library/random.html) , [Numpy](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.RandomState.html#numpy.random.RandomState) ) as if this is not used, every time you run your data split the samples in each set may be different. Therefore, using a random see leads to repeatable results. \n",
    "\n",
    "**TASK**: \n",
    "- Look up how to shuffle your dataframe **not using any libraries (yet)** and shuffle it.\n",
    "- Print out the top few rows of the df and ensure that the PassengerId's are not in order\n",
    "- run the same experiment multiple times and observe that the order changes every time you run it. Then set the random seed using NumPy to 42 and observe that your splits are now reproducible.\n",
    "- Remove the column PassengerID from the dataframe\n",
    "- Then extract the train_df/test_df as you've done above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Sex  Age  Fare  FamilySize  Embarked_C  Embarked_Q  \\\n",
      "0          710         1    1  2.0   2.0           2           1           0   \n",
      "1          440         0    1  2.0   2.0           1           0           0   \n",
      "2          841         0    1  2.0   1.0           1           0           0   \n",
      "3          721         1    0  1.0   2.0           2           0           0   \n",
      "4           40         1    0  1.0   2.0           2           1           0   \n",
      "\n",
      "   Embarked_S  Pclass_1  Pclass_2  Pclass_3  Title_Master  Title_Miss  \\\n",
      "0           0         0         0         1             1           0   \n",
      "1           1         0         1         0             0           0   \n",
      "2           1         0         0         1             0           0   \n",
      "3           1         0         1         0             0           1   \n",
      "4           0         0         0         1             0           1   \n",
      "\n",
      "   Title_Mr  Title_Mrs  Title_Other  Title_th  \n",
      "0         0          0            0         0  \n",
      "1         1          0            0         0  \n",
      "2         1          0            0         0  \n",
      "3         0          0            0         0  \n",
      "4         0          0            0         0  \n",
      "   Survived  Sex  Age  Fare  FamilySize  Embarked_C  Embarked_Q  Embarked_S  \\\n",
      "0         1    1  2.0   2.0           2           1           0           0   \n",
      "1         0    1  2.0   2.0           1           0           0           1   \n",
      "2         0    1  2.0   1.0           1           0           0           1   \n",
      "3         1    0  1.0   2.0           2           0           0           1   \n",
      "4         1    0  1.0   2.0           2           1           0           0   \n",
      "5         1    0  2.0   2.0           1           0           0           1   \n",
      "6         1    0  2.0   1.0           1           0           1           0   \n",
      "7         0    1  1.0   2.0           2           0           0           1   \n",
      "8         1    0  1.0   1.0           1           0           1           0   \n",
      "9         1    0  2.0   2.0           2           0           0           1   \n",
      "\n",
      "   Pclass_1  Pclass_2  Pclass_3  Title_Master  Title_Miss  Title_Mr  \\\n",
      "0         0         0         1             1           0         0   \n",
      "1         0         1         0             0           0         1   \n",
      "2         0         0         1             0           0         1   \n",
      "3         0         1         0             0           1         0   \n",
      "4         0         0         1             0           1         0   \n",
      "5         1         0         0             0           1         0   \n",
      "6         0         0         1             0           1         0   \n",
      "7         0         0         1             0           0         1   \n",
      "8         0         0         1             0           1         0   \n",
      "9         1         0         0             0           1         0   \n",
      "\n",
      "   Title_Mrs  Title_Other  Title_th  \n",
      "0          0            0         0  \n",
      "1          0            0         0  \n",
      "2          0            0         0  \n",
      "3          0            0         0  \n",
      "4          0            0         0  \n",
      "5          0            0         0  \n",
      "6          0            0         0  \n",
      "7          0            0         0  \n",
      "8          0            0         0  \n",
      "9          0            0         0  \n",
      "     Survived  Sex  Age  Fare  FamilySize  Embarked_C  Embarked_Q  Embarked_S  \\\n",
      "712         0    1  2.0   2.0           1           0           0           1   \n",
      "713         0    1  2.0   1.0           1           0           0           1   \n",
      "714         1    1  2.0   2.0           2           1           0           0   \n",
      "715         1    0  2.0   2.0           2           0           0           1   \n",
      "716         1    0  2.0   2.0           2           1           0           0   \n",
      "717         0    1  1.0   2.0           3           0           0           1   \n",
      "718         1    0  2.0   2.0           2           0           0           1   \n",
      "719         1    0  2.0   2.0           2           0           0           1   \n",
      "720         0    1  2.0   2.0           1           0           0           1   \n",
      "721         0    1  2.0   3.0           2           1           0           0   \n",
      "\n",
      "     Pclass_1  Pclass_2  Pclass_3  Title_Master  Title_Miss  Title_Mr  \\\n",
      "712         0         1         0             0           0         1   \n",
      "713         0         0         1             0           0         1   \n",
      "714         1         0         0             0           0         1   \n",
      "715         0         1         0             0           0         0   \n",
      "716         1         0         0             0           0         0   \n",
      "717         0         0         1             1           0         0   \n",
      "718         1         0         0             0           0         0   \n",
      "719         0         0         1             0           0         0   \n",
      "720         0         1         0             0           0         1   \n",
      "721         1         0         0             0           0         1   \n",
      "\n",
      "     Title_Mrs  Title_Other  Title_th  \n",
      "712          0            0         0  \n",
      "713          0            0         0  \n",
      "714          0            0         0  \n",
      "715          1            0         0  \n",
      "716          0            1         0  \n",
      "717          0            0         0  \n",
      "718          1            0         0  \n",
      "719          1            0         0  \n",
      "720          0            0         0  \n",
      "721          0            0         0  \n"
     ]
    }
   ],
   "source": [
    "#TODO: set the numpy random seed to 42\n",
    "np.random.seed(42)\n",
    "\n",
    "#TODO: Shuffle df\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(df.head())\n",
    "\n",
    "# TODO: drop the column PassengerId from df\n",
    "df = df.drop(columns=['PassengerId'])\n",
    "\n",
    "\n",
    "train_df = df[:int(0.8*df.shape[0])]\n",
    "test_df = df[int(0.8*df.shape[0]):]\n",
    "\n",
    "print(train_df.head(10))\n",
    "print(test_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split the data we want to separate the data into features and the according label, target variables. \n",
    "This then allows us to feed a model with the samples and compare their prediction with the relevant ground truth (label).\n",
    "\n",
    "**TASK**: \n",
    "- Extract the target variable *survived* from the dataframes and save it in a new dataframe *y_train* and *y_test* respectively\n",
    "- Drop the target column from the train and test dataframes and save the remaining dataframes in the variables X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract the target column from the train and test dataframes and save them in the variables y_train and y_test\n",
    "# TODO: Drop the target column from the train and test dataframes and save the remaining dataframes in the variables X_train and X_test\n",
    "y_train = train_df['Survived']\n",
    "y_test = test_df['Survived']\n",
    "X_train = train_df.drop(columns=['Survived'])\n",
    "X_test = test_df.drop(columns=['Survived'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also utilize the help of libraries to shuffle data such as in our dataframes and then select an appropriate train/val/test split. They also offer other good utilities.\n",
    "Have a read through the documentation of [sklearns train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "**TASK**: \n",
    "- Ise the main (un-split) dataframe df and extract the features and labels as above and same them in variables X and y.\n",
    "- Then, instead of using a manual version of the train_test split, utilize sklearns version in the code cell below. \n",
    "Note that both options are valid ways, but libraries such as sklearn can make things easier at times. \n",
    "- Then print out the top few lines of each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Sex  Age  Fare  FamilySize  Embarked_C  Embarked_Q  Embarked_S  Pclass_1  \\\n",
      "331    1  1.0   1.0           2           1           0           0         0   \n",
      "733    0  2.0   2.0           2           1           0           0         0   \n",
      "382    0  2.0   3.0           2           0           0           1         1   \n",
      "704    1  2.0   2.0           2           0           0           1         0   \n",
      "813    1  2.0   1.0           1           1           0           0         0   \n",
      "\n",
      "     Pclass_2  Pclass_3  Title_Master  Title_Miss  Title_Mr  Title_Mrs  \\\n",
      "331         0         1             0           0         1          0   \n",
      "733         0         1             0           1         0          0   \n",
      "382         0         0             0           0         0          1   \n",
      "704         1         0             0           0         1          0   \n",
      "813         0         1             0           0         1          0   \n",
      "\n",
      "     Title_Other  Title_th  \n",
      "331            0         0  \n",
      "733            0         0  \n",
      "382            0         0  \n",
      "704            0         0  \n",
      "813            0         0  \n",
      "     Sex  Age  Fare  FamilySize  Embarked_C  Embarked_Q  Embarked_S  Pclass_1  \\\n",
      "709    1  2.0   1.0           1           0           0           1         0   \n",
      "439    1  2.0   1.0           1           0           0           1         0   \n",
      "840    0  1.0   2.0           1           0           0           1         1   \n",
      "720    1  2.0   2.0           1           0           0           1         0   \n",
      "39     1  2.0   1.0           1           0           0           1         0   \n",
      "\n",
      "     Pclass_2  Pclass_3  Title_Master  Title_Miss  Title_Mr  Title_Mrs  \\\n",
      "709         0         1             0           0         1          0   \n",
      "439         1         0             0           0         1          0   \n",
      "840         0         0             0           1         0          0   \n",
      "720         1         0             0           0         1          0   \n",
      "39          0         1             0           0         1          0   \n",
      "\n",
      "     Title_Other  Title_th  \n",
      "709            0         0  \n",
      "439            0         0  \n",
      "840            0         0  \n",
      "720            0         0  \n",
      "39             0         0  \n",
      "331    0\n",
      "733    0\n",
      "382    1\n",
      "704    0\n",
      "813    0\n",
      "Name: Survived, dtype: int64\n",
      "709    0\n",
      "439    0\n",
      "840    1\n",
      "720    0\n",
      "39     0\n",
      "Name: Survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TODO: extract the features and labels as above and same them  in variables X and y\n",
    "X = df.drop(columns=['Survived'])\n",
    "y = df['Survived']\n",
    "#TODO: shuffle and extract an 80/20 split of the data in the dataframe df using sklearn.model_selection.train_test_split\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#TODO: Print out the top few lines of each dataframe to check that the split was correct\n",
    "print(X_train.head())\n",
    "print(X_test.head())\n",
    "print(y_train.head())\n",
    "print(y_test.head())\n",
    "\n",
    "# Automatic Test to check whether your split was correct\n",
    "assert X_train.shape[0] >= 712 and train_df.shape[0] <= 713\n",
    "assert X_test.shape[0] >= 178 and test_df.shape[0] <= 179"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-8) Remaining ML Pipeline Steps\n",
    "Now the data should be ready to be used in the remaining steps of the ML pipeline, which we will cover in the coming weeks. \n",
    "\n",
    "If you still have time in the labs, it is advised to look through the recommended reading. \n",
    "Another term that you should look into to give you some more insigth into data splitting is *stratified sampling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lab Week 5 Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: \n",
    "-\tSelecting a classification algorithm from either: Decision Tree, Random Forest, SVM\n",
    "-\tImplement a grid search using scikit learns GridSearchCV method by selecting a range of suitable hyperparameters and use the X_train, y_train variables of this notebook for training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'degree': 3, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [\n",
    "    {\"kernel\": [\"rbf\"], \"gamma\": [1e-3, 1e-4], \"C\": [1, 10, 100, 1000]},\n",
    "    {\"kernel\": [\"linear\"], \"C\": [1, 10, 100, 1000]},\n",
    "    {\"kernel\": [\"poly\"], \"degree\": [2, 3, 4], \"C\": [1, 10, 100, 1000]},\n",
    "]\n",
    "model = SVC()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model, tuned_parameters, scoring=\"accuracy\",cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'criterion': 'entropy', 'max_depth': 4}\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [\n",
    "    {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [2, 3, 4, 5, 6, 7, 8, 9, 10]},\n",
    "]\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model, tuned_parameters, scoring=\"accuracy\",cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'criterion': 'gini', 'max_depth': 6, 'max_features': 'sqrt', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters =[\n",
    "{\"n_estimators\": [10, 50, 100], \"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [2, 4, 6, 10], \"max_features\": [\"sqrt\", \"log2\"]}\n",
    "]\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model, tuned_parameters, scoring=\"accuracy\",cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: \n",
    "-\tCreate a “baseline” object of the chosen classification algorithm and train it on the full training set. \n",
    "-\tThen use the searched hyperparameters that have led to the best results to create a new algorithm object and fit that to the whole set of training data (without Grid Search). \n",
    "-\tGenerate the results using scikit learns classification_report for both, the “baseline” and “grid-searched” model. Which one performs better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Grid Searched SVM--------------------\n",
      "\n",
      "Performance report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       113\n",
      "           1       0.72      0.62      0.67        66\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.76      0.74      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "--------------------Baseline SVM--------------------\n",
      "\n",
      "Performance report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83       113\n",
      "           1       0.73      0.68      0.70        66\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.77      0.77      0.77       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(20*\"-\"+ \"Grid Searched SVM\" + 20*\"-\")\n",
    "best_svm = SVC(C=100, degree=3, kernel=\"poly\")\n",
    "best_svm.fit(X_train, y_train)\n",
    "predictions = best_svm.predict(X_test)\n",
    "print(\"\\nPerformance report:\\n\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "print(20*\"-\"+ \"Baseline SVM\" + 20*\"-\")\n",
    "baseline_sv = SVC()\n",
    "baseline_sv.fit(X_train, y_train)\n",
    "predictions = baseline_sv.predict(X_test)\n",
    "print(\"\\nPerformance report:\\n\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Grid Searched Decision Tree--------------------\n",
      "\n",
      "Performance report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.83       113\n",
      "           1       0.72      0.70      0.71        66\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.77      0.77      0.77       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n",
      "--------------------Baseline Decision Tree--------------------\n",
      "\n",
      "Performance report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.82       113\n",
      "           1       0.72      0.59      0.65        66\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.73      0.74       179\n",
      "weighted avg       0.76      0.77      0.76       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(20*\"-\"+ \"Grid Searched Decision Tree\" + 20*\"-\")\n",
    "best_DecisionTree = DecisionTreeClassifier(criterion='entropy', max_depth=4)\n",
    "best_DecisionTree.fit(X_train, y_train)\n",
    "predictions = best_DecisionTree.predict(X_test)\n",
    "print(\"\\nPerformance report:\\n\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "print(20*\"-\"+ \"Baseline Decision Tree\" + 20*\"-\")\n",
    "baseline_DecisionTree = DecisionTreeClassifier()\n",
    "baseline_DecisionTree.fit(X_train, y_train)\n",
    "predictions = baseline_DecisionTree.predict(X_test)\n",
    "print(\"\\nPerformance report:\\n\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Grid Searched Random Forest--------------------\n",
      "\n",
      "Performance report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       113\n",
      "           1       0.72      0.62      0.67        66\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.76      0.74      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "--------------------Baseline Random Forest--------------------\n",
      "\n",
      "Performance report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       113\n",
      "           1       0.71      0.61      0.66        66\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.73      0.74       179\n",
      "weighted avg       0.76      0.77      0.76       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(20*\"-\"+ \"Grid Searched Random Forest\" + 20*\"-\")\n",
    "best_RandomForest = RandomForestClassifier(criterion='gini', max_depth=6, max_features='sqrt', n_estimators=100)\n",
    "best_RandomForest.fit(X_train, y_train) \n",
    "predictions = best_RandomForest.predict(X_test)\n",
    "print(\"\\nPerformance report:\\n\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "print(20*\"-\"+ \"Baseline Random Forest\" + 20*\"-\")\n",
    "baseline_RandomForest = RandomForestClassifier()\n",
    "baseline_RandomForest.fit(X_train, y_train)\n",
    "predictions = baseline_RandomForest.predict(X_test)\n",
    "print(\"\\nPerformance report:\\n\")\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('AIML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977335c72b126e3991b9de8b6fc74c7a8bf9097191ab51ecd2769bb8eacdf950"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
