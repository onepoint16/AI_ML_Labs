{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Week 6 : Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load data for binary classification\n",
    "binary_classification_Y_pred = np.load('binary_classification_Y_pred.npy')\n",
    "binary_classification_Y_true = np.load('binary_classification_Y_true.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Our classes are as follows:\n",
    "- Positive Class is 0\n",
    "- Negative class is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    \"\"\"Compute the confusion matrix for binary classification\n",
    "    The target class is assumed to be the positive class. (i.e. 0 is positive, 1 is negative)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        np.array: confusion matrix\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # do not change this line\n",
    "    return np.array([[TP, FP], [FN, TN]])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(binary_classification_Y_true, binary_classification_Y_pred))\n",
    "\n",
    "TP, FP, FN, TN = confusion_matrix(binary_classification_Y_true, binary_classification_Y_pred).ravel()\n",
    "print(f\"True Positives: {TP}\")\n",
    "print(f\"False Positives: {FP}\")\n",
    "print(f\"False Negatives: {FN}\")\n",
    "print(f\"True Negatives: {TN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "\n",
    "SK_TN, SK_FP, SK_FN, SK_TP = sk_confusion_matrix(binary_classification_Y_true, binary_classification_Y_pred, labels=[1,0]).ravel()\n",
    "print(f\"Your True Positives: {TP} - Sklearn True Positives: {SK_TP}\")\n",
    "print(f\"Your False Negatives: {FN} - Sklearn False Negatives: {SK_FN}\")\n",
    "print(f\"Your False Positives: {FP} - Sklearn False Positives: {SK_FP}\")\n",
    "print(f\"Your True Negatives: {TN} - Sklearn True Negatives: {SK_TN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(Y_true, Y_pred):\n",
    "    \"\"\"calculate precision\n",
    "    Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: precision score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "def recall(Y_true, Y_pred):\n",
    "    \"\"\"calculate recall\n",
    "    Recall = True Positive / (True Positive + False Negative)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: recall score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return recall_score\n",
    "\n",
    "def f1_score(Y_true, Y_pred):\n",
    "    \"\"\"calculate f1 score\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return f1_score\n",
    "\n",
    "def accuracy(Y_true, Y_pred):\n",
    "    \"\"\"calculate accuracy\n",
    "    Accuracy = (True Positive + True Negative) / (True Positive + True Negative + False Positive + False Negative)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: accuracy score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "   \n",
    "    return accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for correct results\n",
    "# Checking for correct results\n",
    "\n",
    "from sklearn.metrics import f1_score as sk_f1_score\n",
    "from sklearn.metrics import accuracy_score as sk_accuracy_score\n",
    "from sklearn.metrics import precision_score as sk_precision_score\n",
    "from sklearn.metrics import recall_score as sk_recall_score\n",
    "print(\"Ensure that the output of your functions match that of sklearn.\")\n",
    "print(f\"Your precision score: {precision(binary_classification_Y_true, binary_classification_Y_pred)} - sklearn's precision score: {sk_precision_score(binary_classification_Y_true, binary_classification_Y_pred, pos_label = 0)}\")\n",
    "print(f\"Your recall score: {recall(binary_classification_Y_true, binary_classification_Y_pred)} - sklearn's recall score: {sk_recall_score(binary_classification_Y_true, binary_classification_Y_pred, pos_label = 0)}\")\n",
    "print(f\"Your f1 score: {f1_score(binary_classification_Y_true, binary_classification_Y_pred)} - sklearn's f1 score: {sk_f1_score(binary_classification_Y_true, binary_classification_Y_pred, pos_label = 0)}\")\n",
    "print(f\"Your accuracy score: {accuracy(binary_classification_Y_true, binary_classification_Y_pred)} - sklearn's accuracy score: {sk_accuracy_score(binary_classification_Y_true, binary_classification_Y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_Y_pred = np.load('regression_Y_pred.npy')\n",
    "regression_Y_true = np.load('regression_Y_true.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(Y_true, Y_pred):\n",
    "    \"\"\"calculate mean absolute error\n",
    "    MAE = 1/n * sum(|Y_true - Y_pred|)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: mean absolute error\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return mae_score\n",
    "\n",
    "def mean_squared_error(Y_true, Y_pred):\n",
    "    \"\"\"calculate mean squared error\n",
    "    MSE = 1/n * sum((Y_true - Y_pred)^2)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: mean squared error\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return mse_score\n",
    "\n",
    "def root_mean_squared_error(Y_true, Y_pred):\n",
    "    \"\"\"calculate root mean squared error\n",
    "    RMSE = sqrt(MSE)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: root mean squared error\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return rmse_score\n",
    "\n",
    "def mean_absolute_percentage_error(Y_true, Y_pred):\n",
    "    \"\"\"calculate mean absolute percentage error\n",
    "    MAPE = 1/n * sum(|(Y_true - Y_pred)/Y_true|)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: mean absolute percentage error\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return mape_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for correct results\n",
    "from sklearn.metrics import mean_absolute_error as sk_mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as sk_mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error as sk_mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "print(\"Ensure that the output of your functions match that of sklearn.\")\n",
    "print(f\"Your mean absolute error: {mean_absolute_error(regression_Y_true, regression_Y_pred)} - sklearn's mean absolute error: {sk_mean_absolute_error(regression_Y_true, regression_Y_pred)}\")\n",
    "print(f\"Your mean squared error: {mean_squared_error(regression_Y_true, regression_Y_pred)} - sklearn's mean squared error: {sk_mean_squared_error(regression_Y_true, regression_Y_pred)}\")\n",
    "print(f\"Your root mean squared error: {root_mean_squared_error(regression_Y_true, regression_Y_pred)} - sklearn's root mean squared error: {np.sqrt(sk_mean_squared_error(regression_Y_true, regression_Y_pred))}\")\n",
    "print(f\"Your mean absolute percentage error: {mean_absolute_percentage_error(regression_Y_true, regression_Y_pred)} - sklearn's mean absolute percentage error: {sk_mean_absolute_percentage_error(regression_Y_true, regression_Y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "981baf1948874b62570d8088db1242b0b85753f221a31816c8c587837181a860"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
