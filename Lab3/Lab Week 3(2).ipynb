{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Agents and Environments\n",
    "\n",
    "In the lecture we have seen multiple different agent types. \n",
    "In this lab you will implement the first two:\n",
    "- A simple reflex agent\n",
    "- A model-based reflex agent\n",
    "\n",
    "We will use two environments, one for each agent. \n",
    "\n",
    "In the first one, the simple reflex agent will be playing the game \"TicTacToe\" and in the second, you will implement a model-based agent that can play Rock-Paper-Scissors. \n",
    "\n",
    "*Note*: Those agents will not be learning automatically, you will create them on a rule-based system.\n",
    "We will revisit automatic learning in the later lectures, under the topic: Reinforcement Learning.\n",
    "\n",
    "\n",
    "After this lab you will:\n",
    "- Understand how to implement and use environments (which will be useful when we advance to reinforcement learning)\n",
    "- Have practiced using Numpy and Matplotlib to modify arrays and plot results, which will be handy when working further with machine learning\n",
    "- And if you are still starting off with Python, you will have seen/used Python classes and functions, and should hopefully be also able to implement them in the future\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "Remember from the lecture, environments are the task to which **agent** are the solution. \n",
    "The **agent** takes an **action** (which is the input into the environment), upon which the environment is updated. \n",
    "In reinforcement learning we also have a reward for each step, although we will not use that in this lab. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TicTacToe Environment\n",
    "The following code implements the TicTacToe environment.\n",
    "\n",
    "Read and try to understand the code and its functions. \n",
    "Each environment is initialized with its initial values, and a *step* outlines an agent-interaction with an environment.\n",
    "We then also have a function to reset an environment to its initial state (good for repeating interactions) and also have a get_actions function which lets agent's know what actions they can take.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class TicTacToeEnvironment:\n",
    "    def __init__(self):\n",
    "        self.board = [\" \"] * 9\n",
    "        self.done = False\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, index: int, player):\n",
    "        # take a step in the environment\n",
    "        assert not self.done, \"Game is over\"\n",
    "        assert player in (\"X\", \"O\"), \"Invalid player\"\n",
    "        assert self.board[index] not in (\"X\", \"O\"), \"Square already taken\"\n",
    "\n",
    "        self.board[index] = player\n",
    "\n",
    "        return self.board, self.get_actions()\n",
    "\n",
    "    def get_actions(self) -> list:\n",
    "        return [i for i in range(len(self.board)) if self.board[i] == \" \"]\n",
    "\n",
    "    def reset(self):\n",
    "        # reset the board/environment\n",
    "        self.done = False\n",
    "        self.board = [\" \"] * 9\n",
    "\n",
    "    def check_winner(self) -> str:\n",
    "        # check if there is a winner, returns X, O, D (draw) or N (none) if there is no winner\n",
    "        board = np.reshape(self.board, (3, 3))\n",
    "\n",
    "        self.done = True\n",
    "        # check rows/columns\n",
    "        for b in [board, np.transpose(board)]:\n",
    "            for row in b:\n",
    "                if len(set(row)) == 1 and row[0] != \" \":\n",
    "                    return row[0]\n",
    "\n",
    "        # check diagonals\n",
    "        if len(set([board[i][i] for i in range(3)])) == 1 and board[0][0] != \" \":\n",
    "            return board[0][0]\n",
    "        if len(set([board[i][2 - i] for i in range(3)])) == 1 and board[0][2] != \" \":\n",
    "            return board[0][2]\n",
    "\n",
    "        # check for draw\n",
    "        if len(self.get_actions()) == 0:\n",
    "            return \"Draw\"\n",
    "        self.done = False\n",
    "        return \"N\"\n",
    "\n",
    "    def show_board(self):\n",
    "        print(self.board[0] + \" | \" + self.board[1] + \" | \" + self.board[2])\n",
    "        print(\"---------\")\n",
    "        print(self.board[3] + \" | \" + self.board[4] + \" | \" + self.board[5])\n",
    "        print(\"---------\")\n",
    "        print(self.board[6] + \" | \" + self.board[7] + \" | \" + self.board[8])\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Reflex Agent Interaction with Environment\n",
    "The following code shows how the agent interacts with the environment. Again, try to read and understand the code and how it has been implemented.\n",
    "The agent percept's the board and the action is a turn of an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_ttt(env, agent1, agent2, iterations: int = 100):\n",
    "    # play games of tic tac toe\n",
    "    results = {\"X\": 0, \"O\": 0, \"Draw\": 0}\n",
    "    for i in range(iterations):\n",
    "        # Alternate starting turn between agents\n",
    "        if i % 2 == 0:\n",
    "            turn = agent1\n",
    "            no_turn = agent2\n",
    "        else:\n",
    "            turn = agent2\n",
    "            no_turn = agent1\n",
    "\n",
    "        board = env.board\n",
    "        actions = env.get_actions()\n",
    "\n",
    "        while True:\n",
    "            # play a turn\n",
    "            action, player = turn(board, actions)\n",
    "            board, actions = env.step(action, player)\n",
    "            env.show_board()\n",
    "            # check the winner\n",
    "            winner = env.check_winner()\n",
    "            if winner != \"N\":\n",
    "                print(\"Winner is \" + winner)\n",
    "                results[winner] += 1\n",
    "                env.reset()\n",
    "                break\n",
    "            # switch players\n",
    "            turn, no_turn = no_turn, turn\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Reflex Based Agent\n",
    "As you can see, the agent observes the environment (e.g. the current state of the environment) and then performs an action based upon that. \n",
    "\n",
    "In this basic example, the agent simply picks a random action. This is suboptimal but can serve as a performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(state, actions: list) -> int:\n",
    "    # random agent\n",
    "    return np.random.choice(actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Agent\n",
    "This code implements a manual agent (e.g. let's you interact with the environment). \n",
    "\n",
    "Use this code for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def manual_agent_ttt(state, actions: list) -> int:\n",
    "    clear_output(wait = True)\n",
    "    print(\"-------------------\")\n",
    "    print(\"Current Board\")\n",
    "    print(np.array(state).reshape(3, 3))\n",
    "    print(f\"Available Actions{actions}\")\n",
    "    action = int(input(\"Enter action: \"))\n",
    "    print(f\"Chosen Action:{action}\")\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Agent-Environment Interactions\n",
    "\n",
    "The following code contains two functions. These are wrapper functions to assign an agent the \"X\" or \"O\" player. Change the return statement if you want to switch agents around.\n",
    "The call to the *play_ttt* function lets the agents interact with this environment. In this particular case, it is a Multi-Agent system, as now, several agents are interacting with one another in this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | X |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | X | O\n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "  | X | O\n",
      "\n",
      "  | O |  \n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "  | X | O\n",
      "\n",
      "  | O | X\n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "  | X | O\n",
      "\n",
      "  | O | X\n",
      "---------\n",
      "O | X |  \n",
      "---------\n",
      "  | X | O\n",
      "\n",
      "  | O | X\n",
      "---------\n",
      "O | X |  \n",
      "---------\n",
      "X | X | O\n",
      "\n",
      "Winner is X\n",
      "  |   |  \n",
      "---------\n",
      "O |   |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "O |   |  \n",
      "---------\n",
      "X |   |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "X |   |  \n",
      "\n",
      "  |   | X\n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "X |   |  \n",
      "\n",
      "  |   | X\n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "X |   | O\n",
      "\n",
      "  |   | X\n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "X | X | O\n",
      "\n",
      "  |   | X\n",
      "---------\n",
      "O | O | O\n",
      "---------\n",
      "X | X | O\n",
      "\n",
      "Winner is O\n",
      "  |   |  \n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "  |   | O\n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "  | X | O\n",
      "\n",
      "O |   |  \n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "  | X | O\n",
      "\n",
      "O | X |  \n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "  | X | O\n",
      "\n",
      "Winner is X\n",
      "  | O |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "  | O |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   | X\n",
      "\n",
      "  | O |  \n",
      "---------\n",
      "O |   |  \n",
      "---------\n",
      "  |   | X\n",
      "\n",
      "  | O |  \n",
      "---------\n",
      "O |   |  \n",
      "---------\n",
      "X |   | X\n",
      "\n",
      "  | O |  \n",
      "---------\n",
      "O |   | O\n",
      "---------\n",
      "X |   | X\n",
      "\n",
      "  | O | X\n",
      "---------\n",
      "O |   | O\n",
      "---------\n",
      "X |   | X\n",
      "\n",
      "  | O | X\n",
      "---------\n",
      "O | O | O\n",
      "---------\n",
      "X |   | X\n",
      "\n",
      "Winner is O\n",
      "  |   | X\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "  |   | X\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "  |   | X\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "X | O |  \n",
      "\n",
      "  | O | X\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "X | O |  \n",
      "\n",
      "  | O | X\n",
      "---------\n",
      "X |   |  \n",
      "---------\n",
      "X | O |  \n",
      "\n",
      "  | O | X\n",
      "---------\n",
      "X |   |  \n",
      "---------\n",
      "X | O | O\n",
      "\n",
      "X | O | X\n",
      "---------\n",
      "X |   |  \n",
      "---------\n",
      "X | O | O\n",
      "\n",
      "Winner is X\n",
      "O |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "O |   |  \n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "O | O |  \n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "O | O |  \n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "  |   | X\n",
      "\n",
      "O | O | O\n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "  |   | X\n",
      "\n",
      "Winner is O\n",
      "  |   |  \n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  | X | X\n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "O | X | X\n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "X |   |  \n",
      "---------\n",
      "O | X | X\n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "X | O |  \n",
      "---------\n",
      "O | X | X\n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "X | O |  \n",
      "---------\n",
      "O | X | X\n",
      "---------\n",
      "  | O | X\n",
      "\n",
      "Winner is X\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "  | O |  \n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "  | O | X\n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "  | O | X\n",
      "---------\n",
      "  | O | O\n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "  | O | X\n",
      "---------\n",
      "X | O | O\n",
      "\n",
      "  | X | O\n",
      "---------\n",
      "  | O | X\n",
      "---------\n",
      "X | O | O\n",
      "\n",
      "X | X | O\n",
      "---------\n",
      "  | O | X\n",
      "---------\n",
      "X | O | O\n",
      "\n",
      "X | X | O\n",
      "---------\n",
      "O | O | X\n",
      "---------\n",
      "X | O | O\n",
      "\n",
      "Winner is Draw\n",
      "  |   | X\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "  |   | X\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "X |   | X\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "X |   | X\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "O | O |  \n",
      "\n",
      "X |   | X\n",
      "---------\n",
      "X |   |  \n",
      "---------\n",
      "O | O |  \n",
      "\n",
      "X | O | X\n",
      "---------\n",
      "X |   |  \n",
      "---------\n",
      "O | O |  \n",
      "\n",
      "X | O | X\n",
      "---------\n",
      "X |   |  \n",
      "---------\n",
      "O | O | X\n",
      "\n",
      "X | O | X\n",
      "---------\n",
      "X |   | O\n",
      "---------\n",
      "O | O | X\n",
      "\n",
      "X | O | X\n",
      "---------\n",
      "X | X | O\n",
      "---------\n",
      "O | O | X\n",
      "\n",
      "Winner is X\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "X |   |  \n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "X |   |  \n",
      "---------\n",
      "O | O |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "X | X |  \n",
      "---------\n",
      "O | O |  \n",
      "\n",
      "O |   |  \n",
      "---------\n",
      "X | X |  \n",
      "---------\n",
      "O | O |  \n",
      "\n",
      "O |   |  \n",
      "---------\n",
      "X | X | X\n",
      "---------\n",
      "O | O |  \n",
      "\n",
      "Winner is X\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "X |   |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  | O |  \n",
      "---------\n",
      "X |   |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "X | O |  \n",
      "---------\n",
      "X |   |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "X | O |  \n",
      "---------\n",
      "X | O |  \n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "X | O |  \n",
      "---------\n",
      "X | O |  \n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "X | O |  \n",
      "---------\n",
      "X | O | O\n",
      "\n",
      "X | X |  \n",
      "---------\n",
      "X | O |  \n",
      "---------\n",
      "X | O | O\n",
      "\n",
      "Winner is X\n",
      "  |   |  \n",
      "---------\n",
      "  | O |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  | O |  \n",
      "---------\n",
      "  | X |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "  | X |  \n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "  | X |  \n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "  | X | O\n",
      "\n",
      "  | X | X\n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "  | X | O\n",
      "\n",
      "  | X | X\n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "O | X | O\n",
      "\n",
      "  | X | X\n",
      "---------\n",
      "O | O | X\n",
      "---------\n",
      "O | X | O\n",
      "\n",
      "O | X | X\n",
      "---------\n",
      "O | O | X\n",
      "---------\n",
      "O | X | O\n",
      "\n",
      "Winner is O\n",
      "  | X |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "O | X |  \n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "O | X |  \n",
      "---------\n",
      "  | X | X\n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "O | X |  \n",
      "---------\n",
      "O | X | X\n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "O | X |  \n",
      "---------\n",
      "O | X | X\n",
      "---------\n",
      "X | O |  \n",
      "\n",
      "O | X |  \n",
      "---------\n",
      "O | X | X\n",
      "---------\n",
      "X | O | O\n",
      "\n",
      "O | X | X\n",
      "---------\n",
      "O | X | X\n",
      "---------\n",
      "X | O | O\n",
      "\n",
      "Winner is X\n",
      "{'X': 8, 'O': 4, 'Draw': 1}\n"
     ]
    }
   ],
   "source": [
    "def playerX_ttt(state, actions: list)  -> tuple:\n",
    "    # uses an agent and adds X as the player identifier to the output\n",
    "    return random_agent(state, actions) , \"X\"\n",
    "\n",
    "\n",
    "def playerO_ttt(state, actions: list) -> tuple:\n",
    "    # uses an agent and adds O as the player identifier to the output\n",
    "    return random_agent(state, actions) , \"O\"\n",
    "\n",
    "\n",
    "play_ttt(TicTacToeEnvironment(), playerX_ttt, playerO_ttt, iterations=13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "## General\n",
    "- Run the code above, try to understand it and change some variables such as the iterations in the call to the *play* function\n",
    "- Add further comments to the code if it is not clear (ask the lab tutor if you have problems understanding parts of it)\n",
    "- For any exercises you can add markdown or code blocks as you wish\n",
    "- The more often you let agents *play* against one another, the larger the sample size is. This could provide more accurate statistics for the results. \n",
    "\n",
    "## Exercise 1: Simple Reflex Agent\n",
    "1) Create a new agent function with the same format as the Random Reflex Agent above. Implement a rule-based strategy that always tries to start with placing a mark at a particular location (if available). Test this against the purely random reflex agent. How does this fare?\n",
    "2) Think about the strategy that you would use to win this game. Implement this as a new agent and test how it fares against the previous two, and against itself. \n",
    "3) If you have built a successful agent ion the previous step, see if you can test it against the strategy of one of your peers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | X |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  |   | O\n",
      "---------\n",
      "  | X |  \n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "  |   | O\n",
      "---------\n",
      "  | X |  \n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "  | O | O\n",
      "---------\n",
      "  | X |  \n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "  | O | O\n",
      "---------\n",
      "X | X |  \n",
      "\n",
      "  | X | O\n",
      "---------\n",
      "  | O | O\n",
      "---------\n",
      "X | X |  \n",
      "\n",
      "  | X | O\n",
      "---------\n",
      "  | O | O\n",
      "---------\n",
      "X | X | X\n",
      "\n",
      "Winner is X\n",
      "O |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "O |   |  \n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "O |   |  \n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "O |   | X\n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "O |   | X\n",
      "---------\n",
      "O |   | X\n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "O |   | X\n",
      "---------\n",
      "O |   | X\n",
      "---------\n",
      "X | O |  \n",
      "\n",
      "O |   | X\n",
      "---------\n",
      "O | O | X\n",
      "---------\n",
      "X | O |  \n",
      "\n",
      "O |   | X\n",
      "---------\n",
      "O | O | X\n",
      "---------\n",
      "X | O | X\n",
      "\n",
      "Winner is X\n",
      "  | X |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "O | X |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "O | X |  \n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "O | X |  \n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "  |   | O\n",
      "\n",
      "O | X | X\n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "  |   | O\n",
      "\n",
      "O | X | X\n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "O |   | O\n",
      "\n",
      "O | X | X\n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "O | X | O\n",
      "\n",
      "O | X | X\n",
      "---------\n",
      "  | O | X\n",
      "---------\n",
      "O | X | O\n",
      "\n",
      "Winner is O\n",
      "  |   |  \n",
      "---------\n",
      "  | O |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "X |   |  \n",
      "---------\n",
      "  | O |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "X |   |  \n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "X |   |  \n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "  | X |  \n",
      "\n",
      "X | O |  \n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "  | X |  \n",
      "\n",
      "X | O | X\n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "  | X |  \n",
      "\n",
      "X | O | X\n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "O | X |  \n",
      "\n",
      "X | O | X\n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "O | X | X\n",
      "\n",
      "X | O | X\n",
      "---------\n",
      "O | O | O\n",
      "---------\n",
      "O | X | X\n",
      "\n",
      "Winner is O\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   | X\n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | O | X\n",
      "\n",
      "  |   | X\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | O | X\n",
      "\n",
      "  |   | X\n",
      "---------\n",
      "  |   | O\n",
      "---------\n",
      "  | O | X\n",
      "\n",
      "  | X | X\n",
      "---------\n",
      "  |   | O\n",
      "---------\n",
      "  | O | X\n",
      "\n",
      "O | X | X\n",
      "---------\n",
      "  |   | O\n",
      "---------\n",
      "  | O | X\n",
      "\n",
      "O | X | X\n",
      "---------\n",
      "  |   | O\n",
      "---------\n",
      "X | O | X\n",
      "\n",
      "O | X | X\n",
      "---------\n",
      "O |   | O\n",
      "---------\n",
      "X | O | X\n",
      "\n",
      "O | X | X\n",
      "---------\n",
      "O | X | O\n",
      "---------\n",
      "X | O | X\n",
      "\n",
      "Winner is X\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "  | O |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "  | O | O\n",
      "\n",
      "X |   |  \n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "  | O | O\n",
      "\n",
      "X |   |  \n",
      "---------\n",
      "  | X |  \n",
      "---------\n",
      "O | O | O\n",
      "\n",
      "Winner is O\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   | X\n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  |   | O\n",
      "---------\n",
      "  |   | X\n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  |   | O\n",
      "---------\n",
      "X |   | X\n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  |   | O\n",
      "---------\n",
      "X | O | X\n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "  |   | O\n",
      "---------\n",
      "X | O | X\n",
      "\n",
      "  | X |  \n",
      "---------\n",
      "  | O | O\n",
      "---------\n",
      "X | O | X\n",
      "\n",
      "  | X | X\n",
      "---------\n",
      "  | O | O\n",
      "---------\n",
      "X | O | X\n",
      "\n",
      "O | X | X\n",
      "---------\n",
      "  | O | O\n",
      "---------\n",
      "X | O | X\n",
      "\n",
      "O | X | X\n",
      "---------\n",
      "X | O | O\n",
      "---------\n",
      "X | O | X\n",
      "\n",
      "Winner is Draw\n",
      "  |   | O\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "X |   | O\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "X |   | O\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   | O\n",
      "\n",
      "X |   | O\n",
      "---------\n",
      "X |   |  \n",
      "---------\n",
      "  |   | O\n",
      "\n",
      "X |   | O\n",
      "---------\n",
      "X | O |  \n",
      "---------\n",
      "  |   | O\n",
      "\n",
      "X |   | O\n",
      "---------\n",
      "X | O | X\n",
      "---------\n",
      "  |   | O\n",
      "\n",
      "X |   | O\n",
      "---------\n",
      "X | O | X\n",
      "---------\n",
      "  | O | O\n",
      "\n",
      "X |   | O\n",
      "---------\n",
      "X | O | X\n",
      "---------\n",
      "X | O | O\n",
      "\n",
      "Winner is X\n",
      "  |   | X\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "  |   | X\n",
      "---------\n",
      "O |   |  \n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "  |   | X\n",
      "---------\n",
      "O |   | X\n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "  | O | X\n",
      "---------\n",
      "O |   | X\n",
      "---------\n",
      "  |   |  \n",
      "\n",
      "  | O | X\n",
      "---------\n",
      "O |   | X\n",
      "---------\n",
      "  | X |  \n",
      "\n",
      "  | O | X\n",
      "---------\n",
      "O |   | X\n",
      "---------\n",
      "  | X | O\n",
      "\n",
      "  | O | X\n",
      "---------\n",
      "O |   | X\n",
      "---------\n",
      "X | X | O\n",
      "\n",
      "O | O | X\n",
      "---------\n",
      "O |   | X\n",
      "---------\n",
      "X | X | O\n",
      "\n",
      "O | O | X\n",
      "---------\n",
      "O | X | X\n",
      "---------\n",
      "X | X | O\n",
      "\n",
      "Winner is X\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "O |   |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  |   | X\n",
      "---------\n",
      "O |   |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "  | O | X\n",
      "---------\n",
      "O |   |  \n",
      "\n",
      "  |   |  \n",
      "---------\n",
      "X | O | X\n",
      "---------\n",
      "O |   |  \n",
      "\n",
      "  | O |  \n",
      "---------\n",
      "X | O | X\n",
      "---------\n",
      "O |   |  \n",
      "\n",
      "  | O |  \n",
      "---------\n",
      "X | O | X\n",
      "---------\n",
      "O | X |  \n",
      "\n",
      "  | O | O\n",
      "---------\n",
      "X | O | X\n",
      "---------\n",
      "O | X |  \n",
      "\n",
      "Winner is O\n",
      "{'X': 5, 'O': 4, 'Draw': 1}\n"
     ]
    }
   ],
   "source": [
    "def rule_start_reflex_agent_ttt(state, actions: list) -> int:\n",
    "    # rule-based strategy that always tries to start with placing a mark at a particular location\n",
    "    # TODO: your code for Exercise 1.1 here\n",
    "    # action = np.insert(board, none, [['O']], axis=[1,1])\n",
    "    if 4 in actions:\n",
    "        return 4\n",
    "    else:\n",
    "        return np.random.choice(actions)\n",
    "\n",
    "\n",
    "def your_strategy_reflex_agent_ttt(state, actions: list) -> int:\n",
    "    # TODO: your code for Exercise 1.2 here\n",
    "    if not 4 in actions:\n",
    "        if 0 in actions:\n",
    "            return 0\n",
    "        elif 2 in actions:\n",
    "            return 2\n",
    "        elif 6 in actions:\n",
    "            return 6\n",
    "        elif 8 in actions:\n",
    "            return 8\n",
    "        else:\n",
    "            return np.random.choice(actions)\n",
    "    elif 4 in actions:\n",
    "        return 4\n",
    "    else:\n",
    "        return np.random.choice(actions)\n",
    "\n",
    "\n",
    "def playerX_ttt(state, actions: list) -> tuple:\n",
    "    # uses an agent and adds X as the player identifier to the output\n",
    "    return random_agent(state, actions) , \"X\"\n",
    "\n",
    "\n",
    "def playerOttt(state, actions: list) -> tuple:\n",
    "    # uses an agent and adds O as the player identifier to the output\n",
    "    return rule_start_reflex_agent_ttt(state, actions) , \"O\"\n",
    "\n",
    "\n",
    "play_ttt(TicTacToeEnvironment(), playerX_ttt, playerO_ttt, iterations=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rock-Paper-Scissors Environment\n",
    "The following code partially implements an environment that lets two agents play Rock-Paper-Scissors using Best-Of-Five (whoever wins three rounds first).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ACTION_DCT = {0: \"Rock\", 1: \"Paper\", 2: \"Scissors\"}\n",
    "WIN_DCT = {\"Agent1\":0, \"Agent2\":1}\n",
    "\n",
    "class RockPaperScissorsEnvironment:\n",
    "    def __init__(self):\n",
    "        self.done = False\n",
    "        self.reset()\n",
    "        \n",
    "        self.agent_1_score = 0\n",
    "        self.agent_2_score = 0\n",
    "        self.total_rounds = 5\n",
    "        self.done = False\n",
    "        self.history = []\n",
    "\n",
    "    def step(self, agent_1_action:int, agent_2_action:int):\n",
    "        # TODO: create an assertion that both actions are valid. Either raise an exception (https://docs.python.org/3/tutorial/errors.html) or use assert (https://www.w3schools.com/python/ref_keyword_assert.asp)\n",
    "        # TODO: stop this function form being executed if the game is done. You should use the self.done variable to do this.\n",
    "        # Hint: use the ACTION_DCT variable to translate from int to string\n",
    "        \n",
    "        wins_against = {\n",
    "            \"Paper\": \"Scissors\",\n",
    "            \"Rock\": \"Paper\",\n",
    "            \"Scissors\": \"Rock\"\n",
    "        }\n",
    "\n",
    "        # TODO: create code that checks who wins the round and updates the score accordingly.\n",
    "        # A draw will just incur a repretition of the round, there is no winner. \n",
    "\n",
    "        # TODO: create code that stores the history of the game. You should use the self.history variable for this. The history should be a list of tuples. Each tuple should contain the actions of both agents and the winner of the round. For example: [(0, 1, 1), (1, 0, 2), (2, 2, 0)]\n",
    "        # Note: the history does not need to be updated on a draw\n",
    "        # Hint: use the ACTION_DCT variable to translate from int to string for the winner if you want to parse the history\n",
    "    \n",
    "        self.check_done()\n",
    "\n",
    "        return winner, self.history\n",
    "\n",
    "    def get_actions(self) -> list:\n",
    "        # TODO: return available actions as a list\n",
    "        return None\n",
    "    \n",
    "    def check_done(self):\n",
    "        # TODO: check if the game is done and set the self.done variable accordingly\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        # TODO: code to reset the scores, done and the history \n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def show_history(history):\n",
    "    # helper function to show the history\n",
    "    for ix, h in enumerate(history):\n",
    "        print(f\"Round {ix+1}: Agent 1:{ACTION_DCT[h[0]]}, Agent 2: {ACTION_DCT[h[1]]} -- Winner Agent {h[2]}\")\n",
    "\n",
    "\n",
    "def play_rps(agent_1, agent_2, iterations=100, verbose=False):\n",
    "    \"\"\"Code to play the Rock Paper Scissors Environment\n",
    "\n",
    "    Args:\n",
    "        agent_1 (func): Python function for the first agent\n",
    "        agent_2 (func): Python function for the second agent\n",
    "        iterations (int, optional): How many rounds of best-of-five to play. Defaults to 100.\n",
    "        verbose (bool, optional): Whether to show the result after every round. Defaults to False.\n",
    "    \"\"\"\n",
    "    env = RockPaperScissorsEnvironment()\n",
    "    results = {\"Agent1\": 0, \"Agent2\": 0}\n",
    "    history = []\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        while not env.done:\n",
    "            agent_1_action = agent_1(history, env.get_actions())\n",
    "            agent_2_action = agent_2(history, env.get_actions())\n",
    "            winner, history = env.step(agent_1_action, agent_2_action)\n",
    "\n",
    "        if env.agent_1_score > env.agent_2_score:\n",
    "            results[\"Agent1\"] += 1\n",
    "            winner = \"Agent 1\"\n",
    "        else:\n",
    "            results[\"Agent2\"] += 1\n",
    "            winner = \"Agent 2\"\n",
    "\n",
    "        if verbose:\n",
    "            print(\"#\"*72)\n",
    "            print(f\"Iteration {i+1} Done. Game Summary:\")\n",
    "            show_history(history)\n",
    "            print(f\"Winner: {winner}\")\n",
    "            print(f\"Current Score: Agent 1: {results['Agent1']}, Agent 2: {results['Agent2']}\")\n",
    "            print(\"#\"*72)\n",
    "\n",
    "\n",
    "        \n",
    "        env.reset()\n",
    "\n",
    "    print(results)\n",
    "\n",
    "# run the environment and play two random agents against one another\n",
    "play_rps(random_agent, random_agent, iterations=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Model Based Reflex Agent\n",
    "1) Complete the code above to create the RockPaperScissors Environment.\n",
    "2) Test the environment by creating a manual agent, and run the *play_rps* function with that agent.\n",
    "3) Implement a model-based agent that can play the game. It should look at the history of the game and use this to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_agent_rps(history, actions: list) -> int:\n",
    "    # TODO: your code for Exercise 2.2. here\n",
    "    # Note: modify the code from manual_agent_ttt()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def your_strategy_model_based_reflex_agent_rps(history, actions:list) -> int:\n",
    "    # TODO: your code for Exercise 2.3 here\n",
    "    return None\n",
    "\n",
    "\n",
    "play_rps(random_agent, manual_agent_rps, iterations=1000, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Model Based Recurrent Neural Network Agent (Extra Exercise)\n",
    "1. The code below contains an agent based on recurrent neural networks (RNN's) that has been trained on thousands of games of rock-paper-scissors (adapted from https://github.com/PaulKlinger/rps-rnn). Test (and adapt the strategy) of your model-based agent to play, and win, against this RNN agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to install a tensorflow version >2. in your conda environment to run this RNN Agent\n",
    "# this code does not need to be modified or changed\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_deep_model(state_dims, batch_size, stateful=False):\n",
    "    # source: https://github.com/PaulKlinger/rps-rnn\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.SimpleRNN(state_dims[0], batch_input_shape=[batch_size, None, 6],\n",
    "                                 return_sequences=True,  stateful=stateful, activation=\"softsign\"),\n",
    "    ] + [tf.keras.layers.SimpleRNN(s, return_sequences=True, stateful=stateful, activation=\"softsign\") \n",
    "         for s in state_dims[1:]\n",
    "        ] + [\n",
    "        tf.keras.layers.Dense(3),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ])\n",
    "\n",
    "deep_model_3l = build_deep_model([10,10,10], 1)\n",
    "# make sure you have downloaded the weights from GCU-Learn\n",
    "deep_model_3l.load_weights(\"deep_3l_s10_softsign_sim.h5\")\n",
    "\n",
    "def make_input_vector(opponent_move=None, model_move=None):\n",
    "    # source: https://github.com/PaulKlinger/rps-rnn\n",
    "    if opponent_move is None and model_move is None:\n",
    "        return np.zeros((1,1,6)).astype(np.float32)\n",
    "    elif opponent_move is None or model_move is None:\n",
    "        raise ValueError\n",
    "    move_ids = [[1,0,0],[0,1,0],[0,0,1]]\n",
    "    return np.array([[move_ids[opponent_move] + move_ids[model_move]]]).astype(np.float32)\n",
    "\n",
    "def rnn_agent(history, actions) -> int:\n",
    "    # RNN based agent. Note that this agent needs to be Agent 1 for the environment. \n",
    "\n",
    "    # parse history\n",
    "    if len(history) == 0:\n",
    "        action = np.random.choice(actions)\n",
    "    else:\n",
    "        # convert game history into format for RNN\n",
    "        inp = np.zeros([1,len(history),6], dtype=np.float64)\n",
    "        for ix, h in enumerate(history):\n",
    "            a1, a2, _ = h\n",
    "            inp[0,ix] = make_input_vector(a2, a1)\n",
    "\n",
    "        prediction = deep_model_3l(inp).numpy()\n",
    "\n",
    "        # sample from softmax with temperature to avoid being stuck in an infinite loop if two RNN's play against one another\n",
    "        temperature = 0.1\n",
    "        pred = np.log(prediction[0,-1]) / temperature\n",
    "        exp = np.exp(pred)\n",
    "        pred = exp / np.sum(exp)\n",
    "        action = np.argmax(np.random.multinomial(1, pred, 1)[0])\n",
    "    # make RNN start from scratch every iteration\n",
    "    deep_model_3l.reset_states()\n",
    "    return action\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to test your agent against the RNN agent. \n",
    "# Please note, due to current limitations this agent only works if it is agent1 (e.g. the first function argument)\n",
    "# change the random agent against the model based agent you have built (your_strategy_model_based_reflex_agent_rps()).\n",
    "play_rps(rnn_agent, random_agent, iterations=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-ML-DEV",
   "language": "python",
   "name": "ai-ml-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
